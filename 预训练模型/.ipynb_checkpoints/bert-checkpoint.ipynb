{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hf api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:01.752074Z",
     "start_time": "2021-02-10T06:54:01.665994Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "__version__ = \"4.3.2\"\n",
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "from os.path import expanduser\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "ENDPOINT = \"https://huggingface.co\"\n",
    "\n",
    "\n",
    "class RepoObj:\n",
    "    \"\"\"\n",
    "    HuggingFace git-based system, data structure that represents a file belonging to the current user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, lastModified: str, commit: str, size: int, **kwargs):\n",
    "        self.filename = filename\n",
    "        self.lastModified = lastModified\n",
    "        self.commit = commit\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "class S3Obj:\n",
    "    \"\"\"\n",
    "    HuggingFace S3-based system, data structure that represents a file belonging to the current user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, LastModified: str, ETag: str, Size: int, **kwargs):\n",
    "        self.filename = filename\n",
    "        self.LastModified = LastModified\n",
    "        self.ETag = ETag\n",
    "        self.Size = Size\n",
    "\n",
    "\n",
    "class PresignedUrl:\n",
    "    def __init__(self, write: str, access: str, type: str, **kwargs):\n",
    "        self.write = write\n",
    "        self.access = access\n",
    "        self.type = type  # mime-type to send to S3.\n",
    "\n",
    "\n",
    "class ModelSibling:\n",
    "    \"\"\"\n",
    "    Data structure that represents a public file inside a model, accessible from huggingface.co\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rfilename: str, **kwargs):\n",
    "        self.rfilename = rfilename  # filename relative to the model root\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class ModelInfo:\n",
    "    \"\"\"\n",
    "    Info about a public model accessible from huggingface.co\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modelId: Optional[str] = None,  # id of model\n",
    "        author: Optional[str] = None,\n",
    "        downloads: Optional[int] = None,\n",
    "        tags: List[str] = [],\n",
    "        pipeline_tag: Optional[str] = None,\n",
    "        siblings: Optional[List[Dict]] = None,  # list of files that constitute the model\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.modelId = modelId\n",
    "        self.author = author\n",
    "        self.downloads = downloads\n",
    "        self.tags = tags\n",
    "        self.pipeline_tag = pipeline_tag\n",
    "        self.siblings = [ModelSibling(**x) for x in siblings] if siblings is not None else None\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class HfApi:\n",
    "    ALLOWED_S3_FILE_TYPES = [\"datasets\", \"metrics\"]\n",
    "\n",
    "    def __init__(self, endpoint=None):\n",
    "        self.endpoint = endpoint if endpoint is not None else ENDPOINT\n",
    "\n",
    "    def login(self, username: str, password: str) -> str:\n",
    "        \"\"\"\n",
    "        Call HF API to sign in a user and get a token if credentials are valid.\n",
    "\n",
    "        Outputs: token if credentials are valid\n",
    "\n",
    "        Throws: requests.exceptions.HTTPError if credentials are invalid\n",
    "        \"\"\"\n",
    "        path = \"{}/api/login\".format(self.endpoint)\n",
    "        r = requests.post(path, json={\"username\": username, \"password\": password})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"token\"]\n",
    "\n",
    "    def whoami(self, token: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Call HF API to know \"whoami\"\n",
    "        \"\"\"\n",
    "        path = \"{}/api/whoami\".format(self.endpoint)\n",
    "        r = requests.get(path, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"user\"], d[\"orgs\"]\n",
    "\n",
    "    def logout(self, token: str) -> None:\n",
    "        \"\"\"\n",
    "        Call HF API to log out.\n",
    "        \"\"\"\n",
    "        path = \"{}/api/logout\".format(self.endpoint)\n",
    "        r = requests.post(path, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "\n",
    "    def presign(self, token: str, filetype: str, filename: str, organization: Optional[str] = None) -> PresignedUrl:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to get a presigned url to upload `filename` to S3.\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = f\"{self.endpoint}/api/{filetype}/presign\"\n",
    "        r = requests.post(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"filename\": filename, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return PresignedUrl(**d)\n",
    "\n",
    "    def presign_and_upload(\n",
    "        self, token: str, filetype: str, filename: str, filepath: str, organization: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Get a presigned url, then upload file to S3.\n",
    "\n",
    "        Outputs: url: Read-only url for the stored file on S3.\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        urls = self.presign(token, filetype=filetype, filename=filename, organization=organization)\n",
    "        # streaming upload:\n",
    "        # https://2.python-requests.org/en/master/user/advanced/#streaming-uploads\n",
    "        #\n",
    "        # Even though we presign with the correct content-type,\n",
    "        # the client still has to specify it when uploading the file.\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            pf = TqdmProgressFileReader(f)\n",
    "            data = f if pf.total_size > 0 else \"\"\n",
    "\n",
    "            r = requests.put(urls.write, data=data, headers={\"content-type\": urls.type})\n",
    "            r.raise_for_status()\n",
    "            pf.close()\n",
    "        return urls.access\n",
    "\n",
    "    def list_objs(self, token: str, filetype: str, organization: Optional[str] = None) -> List[S3Obj]:\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to list all stored files for user (or one of their organizations).\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = \"{}/api/{}/listObjs\".format(self.endpoint, filetype)\n",
    "        params = {\"organization\": organization} if organization is not None else None\n",
    "        r = requests.get(path, params=params, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [S3Obj(**x) for x in d]\n",
    "\n",
    "    def delete_obj(self, token: str, filetype: str, filename: str, organization: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        HuggingFace S3-based system, used for datasets and metrics.\n",
    "\n",
    "        Call HF API to delete a file stored by user\n",
    "        \"\"\"\n",
    "        assert filetype in self.ALLOWED_S3_FILE_TYPES, f\"Please specify filetype from {self.ALLOWED_S3_FILE_TYPES}\"\n",
    "        path = \"{}/api/{}/deleteObj\".format(self.endpoint, filetype)\n",
    "        r = requests.delete(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"filename\": filename, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "    def model_list(self) -> List[ModelInfo]:\n",
    "        \"\"\"\n",
    "        Get the public list of all the models on huggingface.co\n",
    "        \"\"\"\n",
    "        path = \"{}/api/models\".format(self.endpoint)\n",
    "        r = requests.get(path)\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [ModelInfo(**x) for x in d]\n",
    "\n",
    "    def list_repos_objs(self, token: str, organization: Optional[str] = None) -> List[RepoObj]:\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to list all stored files for user (or one of their organizations).\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/ls\".format(self.endpoint)\n",
    "        params = {\"organization\": organization} if organization is not None else None\n",
    "        r = requests.get(path, params=params, headers={\"authorization\": \"Bearer {}\".format(token)})\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return [RepoObj(**x) for x in d]\n",
    "\n",
    "    def create_repo(\n",
    "        self,\n",
    "        token: str,\n",
    "        name: str,\n",
    "        organization: Optional[str] = None,\n",
    "        private: Optional[bool] = None,\n",
    "        exist_ok=False,\n",
    "        lfsmultipartthresh: Optional[int] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to create a whole repo.\n",
    "\n",
    "        Params:\n",
    "            private: Whether the model repo should be private (requires a paid huggingface.co account)\n",
    "\n",
    "            exist_ok: Do not raise an error if repo already exists\n",
    "\n",
    "            lfsmultipartthresh: Optional: internal param for testing purposes.\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/create\".format(self.endpoint)\n",
    "        json = {\"name\": name, \"organization\": organization, \"private\": private}\n",
    "        if lfsmultipartthresh is not None:\n",
    "            json[\"lfsmultipartthresh\"] = lfsmultipartthresh\n",
    "        r = requests.post(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json=json,\n",
    "        )\n",
    "        if exist_ok and r.status_code == 409:\n",
    "            return \"\"\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        return d[\"url\"]\n",
    "\n",
    "    def delete_repo(self, token: str, name: str, organization: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        HuggingFace git-based system, used for models.\n",
    "\n",
    "        Call HF API to delete a whole repo.\n",
    "\n",
    "        CAUTION(this is irreversible).\n",
    "        \"\"\"\n",
    "        path = \"{}/api/repos/delete\".format(self.endpoint)\n",
    "        r = requests.delete(\n",
    "            path,\n",
    "            headers={\"authorization\": \"Bearer {}\".format(token)},\n",
    "            json={\"name\": name, \"organization\": organization},\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "class TqdmProgressFileReader:\n",
    "    \"\"\"\n",
    "    Wrap an io.BufferedReader `f` (such as the output of `open(â€¦, \"rb\")`) and override `f.read()` so as to display a\n",
    "    tqdm progress bar.\n",
    "\n",
    "    see github.com/huggingface/transformers/pull/2078#discussion_r354739608 for implementation details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f: io.BufferedReader):\n",
    "        self.f = f\n",
    "        self.total_size = os.fstat(f.fileno()).st_size\n",
    "        self.pbar = tqdm(total=self.total_size, leave=False)\n",
    "        self.read = f.read\n",
    "        f.read = self._read\n",
    "\n",
    "    def _read(self, n=-1):\n",
    "        self.pbar.update(n)\n",
    "        return self.read(n)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "\n",
    "class HfFolder:\n",
    "    path_token = expanduser(\"~/.huggingface/token\")\n",
    "\n",
    "    @classmethod\n",
    "    def save_token(cls, token):\n",
    "        \"\"\"\n",
    "        Save token, creating folder as needed.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(cls.path_token), exist_ok=True)\n",
    "        with open(cls.path_token, \"w+\") as f:\n",
    "            f.write(token)\n",
    "\n",
    "    @classmethod\n",
    "    def get_token(cls):\n",
    "        \"\"\"\n",
    "        Get token or None if not existent.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(cls.path_token, \"r\") as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    @classmethod\n",
    "    def delete_token(cls):\n",
    "        \"\"\"\n",
    "        Delete token. Do not fail if token does not exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.remove(cls.path_token)\n",
    "        except FileNotFoundError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:01.767848Z",
     "start_time": "2021-02-10T06:54:01.754407Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 Optuna, Hugging Face\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Logging utilities. \"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from logging import CRITICAL  # NOQA\n",
    "from logging import DEBUG  # NOQA\n",
    "from logging import ERROR  # NOQA\n",
    "from logging import FATAL  # NOQA\n",
    "from logging import INFO  # NOQA\n",
    "from logging import NOTSET  # NOQA\n",
    "from logging import WARN  # NOQA\n",
    "from logging import WARNING  # NOQA\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "_lock = threading.Lock()\n",
    "_default_handler: Optional[logging.Handler] = None\n",
    "\n",
    "log_levels = {\n",
    "    \"debug\": logging.DEBUG,\n",
    "    \"info\": logging.INFO,\n",
    "    \"warning\": logging.WARNING,\n",
    "    \"error\": logging.ERROR,\n",
    "    \"critical\": logging.CRITICAL,\n",
    "}\n",
    "\n",
    "_default_log_level = logging.WARNING\n",
    "\n",
    "\n",
    "def _get_default_logging_level():\n",
    "    \"\"\"\n",
    "    If TRANSFORMERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n",
    "    not - fall back to ``_default_log_level``\n",
    "    \"\"\"\n",
    "    env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\n",
    "    if env_level_str:\n",
    "        if env_level_str in log_levels:\n",
    "            return log_levels[env_level_str]\n",
    "        else:\n",
    "            logging.getLogger().warning(\n",
    "                f\"Unknown option TRANSFORMERS_VERBOSITY={env_level_str}, \"\n",
    "                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n",
    "            )\n",
    "    return _default_log_level\n",
    "\n",
    "\n",
    "def _get_library_name() -> str:\n",
    "\n",
    "    return __name__.split(\".\")[0]\n",
    "\n",
    "\n",
    "def _get_library_root_logger() -> logging.Logger:\n",
    "\n",
    "    return logging.getLogger(_get_library_name())\n",
    "\n",
    "\n",
    "def _configure_library_root_logger() -> None:\n",
    "\n",
    "    global _default_handler\n",
    "\n",
    "    with _lock:\n",
    "        if _default_handler:\n",
    "            # This library has already configured the library root logger.\n",
    "            return\n",
    "        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n",
    "        _default_handler.flush = sys.stderr.flush\n",
    "\n",
    "        # Apply our default configuration to the library root logger.\n",
    "        library_root_logger = _get_library_root_logger()\n",
    "        library_root_logger.addHandler(_default_handler)\n",
    "        library_root_logger.setLevel(_get_default_logging_level())\n",
    "        library_root_logger.propagate = False\n",
    "\n",
    "\n",
    "def _reset_library_root_logger() -> None:\n",
    "\n",
    "    global _default_handler\n",
    "\n",
    "    with _lock:\n",
    "        if not _default_handler:\n",
    "            return\n",
    "\n",
    "        library_root_logger = _get_library_root_logger()\n",
    "        library_root_logger.removeHandler(_default_handler)\n",
    "        library_root_logger.setLevel(logging.NOTSET)\n",
    "        _default_handler = None\n",
    "\n",
    "\n",
    "def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Return a logger with the specified name.\n",
    "\n",
    "    This function is not supposed to be directly accessed unless you are writing a custom transformers module.\n",
    "    \"\"\"\n",
    "\n",
    "    if name is None:\n",
    "        name = _get_library_name()\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    return logging.getLogger(name)\n",
    "\n",
    "\n",
    "def get_verbosity() -> int:\n",
    "    \"\"\"\n",
    "    Return the current level for the ðŸ¤— Transformers's root logger as an int.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`int`: The logging level.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        ðŸ¤— Transformers has following logging levels:\n",
    "\n",
    "        - 50: ``transformers.logging.CRITICAL`` or ``transformers.logging.FATAL``\n",
    "        - 40: ``transformers.logging.ERROR``\n",
    "        - 30: ``transformers.logging.WARNING`` or ``transformers.logging.WARN``\n",
    "        - 20: ``transformers.logging.INFO``\n",
    "        - 10: ``transformers.logging.DEBUG``\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    return _get_library_root_logger().getEffectiveLevel()\n",
    "\n",
    "\n",
    "def set_verbosity(verbosity: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the vebosity level for the ðŸ¤— Transformers's root logger.\n",
    "\n",
    "    Args:\n",
    "        verbosity (:obj:`int`):\n",
    "            Logging level, e.g., one of:\n",
    "\n",
    "            - ``transformers.logging.CRITICAL`` or ``transformers.logging.FATAL``\n",
    "            - ``transformers.logging.ERROR``\n",
    "            - ``transformers.logging.WARNING`` or ``transformers.logging.WARN``\n",
    "            - ``transformers.logging.INFO``\n",
    "            - ``transformers.logging.DEBUG``\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().setLevel(verbosity)\n",
    "\n",
    "\n",
    "def set_verbosity_info():\n",
    "    \"\"\"Set the verbosity to the :obj:`INFO` level.\"\"\"\n",
    "    return set_verbosity(INFO)\n",
    "\n",
    "\n",
    "def set_verbosity_warning():\n",
    "    \"\"\"Set the verbosity to the :obj:`WARNING` level.\"\"\"\n",
    "    return set_verbosity(WARNING)\n",
    "\n",
    "\n",
    "def set_verbosity_debug():\n",
    "    \"\"\"Set the verbosity to the :obj:`DEBUG` level.\"\"\"\n",
    "    return set_verbosity(DEBUG)\n",
    "\n",
    "\n",
    "def set_verbosity_error():\n",
    "    \"\"\"Set the verbosity to the :obj:`ERROR` level.\"\"\"\n",
    "    return set_verbosity(ERROR)\n",
    "\n",
    "\n",
    "def disable_default_handler() -> None:\n",
    "    \"\"\"Disable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "\n",
    "    assert _default_handler is not None\n",
    "    _get_library_root_logger().removeHandler(_default_handler)\n",
    "\n",
    "\n",
    "def enable_default_handler() -> None:\n",
    "    \"\"\"Enable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "\n",
    "    assert _default_handler is not None\n",
    "    _get_library_root_logger().addHandler(_default_handler)\n",
    "\n",
    "\n",
    "def disable_propagation() -> None:\n",
    "    \"\"\"\n",
    "    Disable propagation of the library log outputs. Note that log propagation is disabled by default.\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().propagate = False\n",
    "\n",
    "\n",
    "def enable_propagation() -> None:\n",
    "    \"\"\"\n",
    "    Enable propagation of the library log outputs. Please disable the HuggingFace Transformers's default handler to\n",
    "    prevent double logging if the root logger has been configured.\n",
    "    \"\"\"\n",
    "\n",
    "    _configure_library_root_logger()\n",
    "    _get_library_root_logger().propagate = True\n",
    "\n",
    "\n",
    "def enable_explicit_format() -> None:\n",
    "    \"\"\"\n",
    "    Enable explicit formatting for every HuggingFace Transformers's logger. The explicit formatter is as follows:\n",
    "\n",
    "    ::\n",
    "\n",
    "        [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE\n",
    "\n",
    "    All handlers currently bound to the root logger are affected by this method.\n",
    "    \"\"\"\n",
    "    handlers = _get_library_root_logger().handlers\n",
    "\n",
    "    for handler in handlers:\n",
    "        formatter = logging.Formatter(\"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\")\n",
    "        handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "def reset_format() -> None:\n",
    "    \"\"\"\n",
    "    Resets the formatting for HuggingFace Transformers's loggers.\n",
    "\n",
    "    All handlers currently bound to the root logger are affected by this method.\n",
    "    \"\"\"\n",
    "    handlers = _get_library_root_logger().handlers\n",
    "\n",
    "    for handler in handlers:\n",
    "        handler.setFormatter(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  file util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.135644Z",
     "start_time": "2021-02-10T06:54:01.770912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team, the AllenNLP library authors. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Utilities for working with the local dataset cache. Parts of this file is adapted from the AllenNLP library at\n",
    "https://github.com/allenai/allennlp.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import fnmatch\n",
    "import importlib.util\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import fields\n",
    "from functools import partial, wraps\n",
    "from hashlib import sha256\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Any, BinaryIO, Dict, List, Optional, Tuple, Union\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import requests\n",
    "from filelock import FileLock\n",
    "\n",
    "# from . import __version__\n",
    "# from .hf_api import HfFolder\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "# The package importlib_metadata is in a different place, depending on the python version.\n",
    "if sys.version_info < (3, 8):\n",
    "    import importlib_metadata\n",
    "else:\n",
    "    import importlib.metadata as importlib_metadata\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n",
    "ENV_VARS_TRUE_AND_AUTO_VALUES = ENV_VARS_TRUE_VALUES.union({\"AUTO\"})\n",
    "\n",
    "USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "USE_JAX = os.environ.get(\"USE_FLAX\", \"AUTO\").upper()\n",
    "\n",
    "if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:\n",
    "    _torch_available = importlib.util.find_spec(\"torch\") is not None\n",
    "    if _torch_available:\n",
    "        try:\n",
    "            _torch_version = importlib_metadata.version(\"torch\")\n",
    "            logger.info(f\"PyTorch version {_torch_version} available.\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            _torch_available = False\n",
    "else:\n",
    "    logger.info(\"Disabling PyTorch because USE_TF is set\")\n",
    "    _torch_available = False\n",
    "\n",
    "\n",
    "if USE_TF in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TORCH not in ENV_VARS_TRUE_VALUES:\n",
    "    _tf_available = importlib.util.find_spec(\"tensorflow\") is not None\n",
    "    if _tf_available:\n",
    "        # For the metadata, we have to look for both tensorflow and tensorflow-cpu\n",
    "        try:\n",
    "            _tf_version = importlib_metadata.version(\"tensorflow\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            try:\n",
    "                _tf_version = importlib_metadata.version(\"tensorflow-cpu\")\n",
    "            except importlib_metadata.PackageNotFoundError:\n",
    "                try:\n",
    "                    _tf_version = importlib_metadata.version(\"tensorflow-gpu\")\n",
    "                except importlib_metadata.PackageNotFoundError:\n",
    "                    try:\n",
    "                        _tf_version = importlib_metadata.version(\"tf-nightly\")\n",
    "                    except importlib_metadata.PackageNotFoundError:\n",
    "                        try:\n",
    "                            _tf_version = importlib_metadata.version(\"tf-nightly-cpu\")\n",
    "                        except importlib_metadata.PackageNotFoundError:\n",
    "                            try:\n",
    "                                _tf_version = importlib_metadata.version(\"tf-nightly-gpu\")\n",
    "                            except importlib_metadata.PackageNotFoundError:\n",
    "                                _tf_version = None\n",
    "                                _tf_available = False\n",
    "    if _tf_available:\n",
    "        if version.parse(_tf_version) < version.parse(\"2\"):\n",
    "            logger.info(f\"TensorFlow found but with version {_tf_version}. Transformers requires version 2 minimum.\")\n",
    "            _tf_available = False\n",
    "        else:\n",
    "            logger.info(f\"TensorFlow version {_tf_version} available.\")\n",
    "else:\n",
    "    logger.info(\"Disabling Tensorflow because USE_TORCH is set\")\n",
    "    _tf_available = False\n",
    "\n",
    "\n",
    "if USE_JAX in ENV_VARS_TRUE_AND_AUTO_VALUES:\n",
    "    _flax_available = importlib.util.find_spec(\"jax\") is not None and importlib.util.find_spec(\"flax\") is not None\n",
    "    if _flax_available:\n",
    "        try:\n",
    "            _jax_version = importlib_metadata.version(\"jax\")\n",
    "            _flax_version = importlib_metadata.version(\"flax\")\n",
    "            logger.info(f\"JAX version {_jax_version}, Flax version {_flax_version} available.\")\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            _flax_available = False\n",
    "else:\n",
    "    _flax_available = False\n",
    "\n",
    "\n",
    "_datasets_available = importlib.util.find_spec(\"datasets\") is not None\n",
    "try:\n",
    "    # Check we're not importing a \"datasets\" directory somewhere but the actual library by trying to grab the version\n",
    "    # AND checking it has an author field in the metadata that is HuggingFace.\n",
    "    _ = importlib_metadata.version(\"datasets\")\n",
    "    _datasets_metadata = importlib_metadata.metadata(\"datasets\")\n",
    "    if _datasets_metadata.get(\"author\", \"\") != \"HuggingFace Inc.\":\n",
    "        _datasets_available = False\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _datasets_available = False\n",
    "\n",
    "\n",
    "_faiss_available = importlib.util.find_spec(\"faiss\") is not None\n",
    "try:\n",
    "    _faiss_version = importlib_metadata.version(\"faiss\")\n",
    "    logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    try:\n",
    "        _faiss_version = importlib_metadata.version(\"faiss-cpu\")\n",
    "        logger.debug(f\"Successfully imported faiss version {_faiss_version}\")\n",
    "    except importlib_metadata.PackageNotFoundError:\n",
    "        _faiss_available = False\n",
    "\n",
    "\n",
    "_scatter_available = importlib.util.find_spec(\"torch_scatter\") is not None\n",
    "try:\n",
    "    _scatter_version = importlib_metadata.version(\"torch_scatter\")\n",
    "    logger.debug(f\"Successfully imported torch-scatter version {_scatter_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _scatter_available = False\n",
    "\n",
    "\n",
    "_soundfile_available = importlib.util.find_spec(\"soundfile\") is not None\n",
    "try:\n",
    "    _soundfile_version = importlib_metadata.version(\"soundfile\")\n",
    "    logger.debug(f\"Successfully imported soundfile version {_soundfile_version}\")\n",
    "except importlib_metadata.PackageNotFoundError:\n",
    "    _soundfile_available = False\n",
    "\n",
    "\n",
    "torch_cache_home = os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n",
    "old_default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n",
    "# New default cache, shared with the Datasets library\n",
    "hf_cache_home = os.path.expanduser(\n",
    "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
    ")\n",
    "default_cache_path = os.path.join(hf_cache_home, \"transformers\")\n",
    "\n",
    "# Onetime move from the old location to the new one if no ENV variable has been set.\n",
    "if (\n",
    "    os.path.isdir(old_default_cache_path)\n",
    "    and not os.path.isdir(default_cache_path)\n",
    "    and \"PYTORCH_PRETRAINED_BERT_CACHE\" not in os.environ\n",
    "    and \"PYTORCH_TRANSFORMERS_CACHE\" not in os.environ\n",
    "    and \"TRANSFORMERS_CACHE\" not in os.environ\n",
    "):\n",
    "    logger.warn(\n",
    "        \"In Transformers v4.0.0, the default path to cache downloaded models changed from \"\n",
    "        \"'~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden \"\n",
    "        \"and '~/.cache/torch/transformers' is a directory that exists, we're moving it to \"\n",
    "        \"'~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should \"\n",
    "        \"only see this message once.\"\n",
    "    )\n",
    "    shutil.move(old_default_cache_path, default_cache_path)\n",
    "\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n",
    "PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n",
    "TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n",
    "\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "TF2_WEIGHTS_NAME = \"tf_model.h5\"\n",
    "TF_WEIGHTS_NAME = \"model.ckpt\"\n",
    "FLAX_WEIGHTS_NAME = \"flax_model.msgpack\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "MODEL_CARD_NAME = \"modelcard.json\"\n",
    "\n",
    "SENTENCEPIECE_UNDERLINE = \"â–\"\n",
    "SPIECE_UNDERLINE = SENTENCEPIECE_UNDERLINE  # Kept for backward compatibility\n",
    "\n",
    "MULTIPLE_CHOICE_DUMMY_INPUTS = [\n",
    "    [[0, 1, 0, 1], [1, 0, 0, 1]]\n",
    "] * 2  # Needs to have 0s and 1s only since XLM uses it for langs too.\n",
    "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\n",
    "DUMMY_MASK = [[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
    "\n",
    "S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n",
    "CLOUDFRONT_DISTRIB_PREFIX = \"https://cdn.huggingface.co\"\n",
    "HUGGINGFACE_CO_PREFIX = \"https://huggingface.co/{model_id}/resolve/{revision}/{filename}\"\n",
    "\n",
    "PRESET_MIRROR_DICT = {\n",
    "    \"tuna\": \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face-models\",\n",
    "    \"bfsu\": \"https://mirrors.bfsu.edu.cn/hugging-face-models\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_torch_available():\n",
    "    return _torch_available\n",
    "\n",
    "\n",
    "def is_tf_available():\n",
    "    return _tf_available\n",
    "\n",
    "\n",
    "def is_flax_available():\n",
    "    return _flax_available\n",
    "\n",
    "\n",
    "def is_torch_tpu_available():\n",
    "    if not _torch_available:\n",
    "        return False\n",
    "    # This test is probably enough, but just in case, we unpack a bit.\n",
    "    if importlib.util.find_spec(\"torch_xla\") is None:\n",
    "        return False\n",
    "    if importlib.util.find_spec(\"torch_xla.core\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"torch_xla.core.xla_model\") is not None\n",
    "\n",
    "\n",
    "def is_datasets_available():\n",
    "    return _datasets_available\n",
    "\n",
    "\n",
    "def is_psutil_available():\n",
    "    return importlib.util.find_spec(\"psutil\") is not None\n",
    "\n",
    "\n",
    "def is_py3nvml_available():\n",
    "    return importlib.util.find_spec(\"py3nvml\") is not None\n",
    "\n",
    "\n",
    "def is_apex_available():\n",
    "    return importlib.util.find_spec(\"apex\") is not None\n",
    "\n",
    "\n",
    "def is_faiss_available():\n",
    "    return _faiss_available\n",
    "\n",
    "\n",
    "def is_sklearn_available():\n",
    "    if importlib.util.find_spec(\"sklearn\") is None:\n",
    "        return False\n",
    "    if importlib.util.find_spec(\"scipy\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"sklearn.metrics\") and importlib.util.find_spec(\"scipy.stats\")\n",
    "\n",
    "\n",
    "def is_sentencepiece_available():\n",
    "    return importlib.util.find_spec(\"sentencepiece\") is not None\n",
    "\n",
    "\n",
    "def is_protobuf_available():\n",
    "    if importlib.util.find_spec(\"google\") is None:\n",
    "        return False\n",
    "    return importlib.util.find_spec(\"google.protobuf\") is not None\n",
    "\n",
    "\n",
    "def is_tokenizers_available():\n",
    "    return importlib.util.find_spec(\"tokenizers\") is not None\n",
    "\n",
    "\n",
    "def is_in_notebook():\n",
    "    try:\n",
    "        # Test adapted from tqdm.autonotebook: https://github.com/tqdm/tqdm/blob/master/tqdm/autonotebook.py\n",
    "        get_ipython = sys.modules[\"IPython\"].get_ipython\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            raise ImportError(\"console\")\n",
    "        if \"VSCODE_PID\" in os.environ:\n",
    "            raise ImportError(\"vscode\")\n",
    "\n",
    "        return importlib.util.find_spec(\"IPython\") is not None\n",
    "    except (AttributeError, ImportError, KeyError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_scatter_available():\n",
    "    return _scatter_available\n",
    "\n",
    "\n",
    "def is_pandas_available():\n",
    "    return importlib.util.find_spec(\"pandas\") is not None\n",
    "\n",
    "\n",
    "def is_sagemaker_distributed_available():\n",
    "    # Get the sagemaker specific env variable.\n",
    "    sagemaker_params = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n",
    "    try:\n",
    "        # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
    "        sagemaker_params = json.loads(sagemaker_params)\n",
    "        if not sagemaker_params.get(\"sagemaker_distributed_dataparallel_enabled\", False):\n",
    "            return False\n",
    "    except json.JSONDecodeError:\n",
    "        return False\n",
    "    # Lastly, check if the `smdistributed` module is present.\n",
    "    return importlib.util.find_spec(\"smdistributed\") is not None\n",
    "\n",
    "\n",
    "def is_soundfile_availble():\n",
    "    return _soundfile_available\n",
    "\n",
    "\n",
    "def torch_only_method(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if not _torch_available:\n",
    "            raise ImportError(\n",
    "                \"You need to install pytorch to use this method or class, \"\n",
    "                \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n",
    "            )\n",
    "        else:\n",
    "            return fn(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "DATASETS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the ðŸ¤— Datasets library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install datasets\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install datasets\n",
    "```\n",
    "then restarting your kernel.\n",
    "\n",
    "Note that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\n",
    "working directory, python may try to import this instead of the ðŸ¤— Datasets library. You should rename this folder or\n",
    "that python file if that's the case.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "TOKENIZERS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the ðŸ¤— Tokenizers library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install tokenizers\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install tokenizers\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SENTENCEPIECE_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PROTOBUF_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "FAISS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\n",
    "that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PYTORCH_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SKLEARN_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the scikit-learn library but it was not found in your environment. You can install it with:\n",
    "```\n",
    "pip install -U scikit-learn\n",
    "```\n",
    "In a notebook or a colab, you can install it by executing a cell with\n",
    "```\n",
    "!pip install -U scikit-learn\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "TENSORFLOW_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "FLAX_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the\n",
    "installation page: https://github.com/google/flax and follow the ones that match your environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "SCATTER_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the torch-scatter library but it was not found in your environment. You can install it with pip as\n",
    "explained here: https://github.com/rusty1s/pytorch_scatter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# docstyle-ignore\n",
    "PANDAS_IMPORT_ERROR = \"\"\"\n",
    "{0} requires the pandas library but it was not found in your environment. You can install it with pip as\n",
    "explained here: https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def requires_datasets(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_datasets_available():\n",
    "        raise ImportError(DATASETS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_faiss(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_faiss_available():\n",
    "        raise ImportError(FAISS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_pytorch(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_torch_available():\n",
    "        raise ImportError(PYTORCH_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_sklearn(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_sklearn_available():\n",
    "        raise ImportError(SKLEARN_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_tf(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_tf_available():\n",
    "        raise ImportError(TENSORFLOW_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_flax(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_flax_available():\n",
    "        raise ImportError(FLAX_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_tokenizers(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_tokenizers_available():\n",
    "        raise ImportError(TOKENIZERS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_sentencepiece(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_sentencepiece_available():\n",
    "        raise ImportError(SENTENCEPIECE_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_protobuf(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_protobuf_available():\n",
    "        raise ImportError(PROTOBUF_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_pandas(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_pandas_available():\n",
    "        raise ImportError(PANDAS_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def requires_scatter(obj):\n",
    "    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n",
    "    if not is_scatter_available():\n",
    "        raise ImportError(SCATTER_IMPORT_ERROR.format(name))\n",
    "\n",
    "\n",
    "def add_start_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_start_docstrings_to_model_forward(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        class_name = \":class:`~transformers.{}`\".format(fn.__qualname__.split(\".\")[0])\n",
    "        intro = \"   The {} forward method, overrides the :func:`__call__` special method.\".format(class_name)\n",
    "        note = r\"\"\"\n",
    "\n",
    "    .. note::\n",
    "        Although the recipe for forward pass needs to be defined within this function, one should call the\n",
    "        :class:`Module` instance afterwards instead of this since the former takes care of running the pre and post\n",
    "        processing steps while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        fn.__doc__ = intro + note + \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_end_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = fn.__doc__ + \"\".join(docstr)\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "PT_RETURN_INTRODUCTION = r\"\"\"\n",
    "    Returns:\n",
    "        :class:`~{full_output_type}` or :obj:`tuple(torch.FloatTensor)`: A :class:`~{full_output_type}` (if\n",
    "        ``return_dict=True`` is passed or when ``config.return_dict=True``) or a tuple of :obj:`torch.FloatTensor`\n",
    "        comprising various elements depending on the configuration (:class:`~transformers.{config_class}`) and inputs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "TF_RETURN_INTRODUCTION = r\"\"\"\n",
    "    Returns:\n",
    "        :class:`~{full_output_type}` or :obj:`tuple(tf.Tensor)`: A :class:`~{full_output_type}` (if\n",
    "        ``return_dict=True`` is passed or when ``config.return_dict=True``) or a tuple of :obj:`tf.Tensor` comprising\n",
    "        various elements depending on the configuration (:class:`~transformers.{config_class}`) and inputs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _get_indent(t):\n",
    "    \"\"\"Returns the indentation in the first line of t\"\"\"\n",
    "    search = re.search(r\"^(\\s*)\\S\", t)\n",
    "    return \"\" if search is None else search.groups()[0]\n",
    "\n",
    "\n",
    "def _convert_output_args_doc(output_args_doc):\n",
    "    \"\"\"Convert output_args_doc to display properly.\"\"\"\n",
    "    # Split output_arg_doc in blocks argument/description\n",
    "    indent = _get_indent(output_args_doc)\n",
    "    blocks = []\n",
    "    current_block = \"\"\n",
    "    for line in output_args_doc.split(\"\\n\"):\n",
    "        # If the indent is the same as the beginning, the line is the name of new arg.\n",
    "        if _get_indent(line) == indent:\n",
    "            if len(current_block) > 0:\n",
    "                blocks.append(current_block[:-1])\n",
    "            current_block = f\"{line}\\n\"\n",
    "        else:\n",
    "            # Otherwise it's part of the description of the current arg.\n",
    "            # We need to remove 2 spaces to the indentation.\n",
    "            current_block += f\"{line[2:]}\\n\"\n",
    "    blocks.append(current_block[:-1])\n",
    "\n",
    "    # Format each block for proper rendering\n",
    "    for i in range(len(blocks)):\n",
    "        blocks[i] = re.sub(r\"^(\\s+)(\\S+)(\\s+)\", r\"\\1- **\\2**\\3\", blocks[i])\n",
    "        blocks[i] = re.sub(r\":\\s*\\n\\s*(\\S)\", r\" -- \\1\", blocks[i])\n",
    "\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "def _prepare_output_docstrings(output_type, config_class):\n",
    "    \"\"\"\n",
    "    Prepares the return part of the docstring using `output_type`.\n",
    "    \"\"\"\n",
    "    docstrings = output_type.__doc__\n",
    "\n",
    "    # Remove the head of the docstring to keep the list of args only\n",
    "    lines = docstrings.split(\"\\n\")\n",
    "    i = 0\n",
    "    while i < len(lines) and re.search(r\"^\\s*(Args|Parameters):\\s*$\", lines[i]) is None:\n",
    "        i += 1\n",
    "    if i < len(lines):\n",
    "        docstrings = \"\\n\".join(lines[(i + 1) :])\n",
    "        docstrings = _convert_output_args_doc(docstrings)\n",
    "\n",
    "    # Add the return introduction\n",
    "    full_output_type = f\"{output_type.__module__}.{output_type.__name__}\"\n",
    "    intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION\n",
    "    intro = intro.format(full_output_type=full_output_type, config_class=config_class)\n",
    "    return intro + docstrings\n",
    "\n",
    "\n",
    "PT_TOKEN_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_QUESTION_ANSWERING_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "        >>> inputs = tokenizer(question, text, return_tensors='pt')\n",
    "        >>> start_positions = torch.tensor([1])\n",
    "        >>> end_positions = torch.tensor([3])\n",
    "\n",
    "        >>> outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> start_scores = outputs.start_logits\n",
    "        >>> end_scores = outputs.end_logits\n",
    "\"\"\"\n",
    "\n",
    "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_MASKED_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"The capital of France is {mask}.\", return_tensors=\"pt\")\n",
    "        >>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_BASE_MODEL_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "\"\"\"\n",
    "\n",
    "PT_MULTIPLE_CHOICE_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "        >>> choice0 = \"It is eaten with a fork and a knife.\"\n",
    "        >>> choice1 = \"It is eaten while held in the hand.\"\n",
    "        >>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "        >>> encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\n",
    "        >>> outputs = model(**{{k: v.unsqueeze(0) for k,v in encoding.items()}}, labels=labels)  # batch size is 1\n",
    "\n",
    "        >>> # the linear classifier still needs to be trained\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "PT_CAUSAL_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> import torch\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_TOKEN_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> input_ids = inputs[\"input_ids\"]\n",
    "        >>> inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_QUESTION_ANSWERING_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "        >>> input_dict = tokenizer(question, text, return_tensors='tf')\n",
    "        >>> outputs = model(input_dict)\n",
    "        >>> start_logits = outputs.start_logits\n",
    "        >>> end_logits = outputs.end_logits\n",
    "\n",
    "        >>> all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "        >>> answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])\n",
    "\"\"\"\n",
    "\n",
    "TF_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_MASKED_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"The capital of France is {mask}.\", return_tensors=\"tf\")\n",
    "        >>> inputs[\"labels\"] = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n",
    "\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_BASE_MODEL_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> outputs = model(inputs)\n",
    "\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "\"\"\"\n",
    "\n",
    "TF_MULTIPLE_CHOICE_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "        >>> choice0 = \"It is eaten with a fork and a knife.\"\n",
    "        >>> choice1 = \"It is eaten while held in the hand.\"\n",
    "\n",
    "        >>> encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='tf', padding=True)\n",
    "        >>> inputs = {{k: tf.expand_dims(v, 0) for k, v in encoding.items()}}\n",
    "        >>> outputs = model(inputs)  # batch size is 1\n",
    "\n",
    "        >>> # the linear classifier still needs to be trained\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "TF_CAUSAL_LM_SAMPLE = r\"\"\"\n",
    "    Example::\n",
    "\n",
    "        >>> from transformers import {tokenizer_class}, {model_class}\n",
    "        >>> import tensorflow as tf\n",
    "\n",
    "        >>> tokenizer = {tokenizer_class}.from_pretrained('{checkpoint}')\n",
    "        >>> model = {model_class}.from_pretrained('{checkpoint}')\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> logits = outputs.logits\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_code_sample_docstrings(\n",
    "    *docstr, tokenizer_class=None, checkpoint=None, output_type=None, config_class=None, mask=None\n",
    "):\n",
    "    def docstring_decorator(fn):\n",
    "        model_class = fn.__qualname__.split(\".\")[0]\n",
    "        is_tf_class = model_class[:2] == \"TF\"\n",
    "        doc_kwargs = dict(model_class=model_class, tokenizer_class=tokenizer_class, checkpoint=checkpoint)\n",
    "\n",
    "        if \"SequenceClassification\" in model_class:\n",
    "            code_sample = TF_SEQUENCE_CLASSIFICATION_SAMPLE if is_tf_class else PT_SEQUENCE_CLASSIFICATION_SAMPLE\n",
    "        elif \"QuestionAnswering\" in model_class:\n",
    "            code_sample = TF_QUESTION_ANSWERING_SAMPLE if is_tf_class else PT_QUESTION_ANSWERING_SAMPLE\n",
    "        elif \"TokenClassification\" in model_class:\n",
    "            code_sample = TF_TOKEN_CLASSIFICATION_SAMPLE if is_tf_class else PT_TOKEN_CLASSIFICATION_SAMPLE\n",
    "        elif \"MultipleChoice\" in model_class:\n",
    "            code_sample = TF_MULTIPLE_CHOICE_SAMPLE if is_tf_class else PT_MULTIPLE_CHOICE_SAMPLE\n",
    "        elif \"MaskedLM\" in model_class or model_class in [\"FlaubertWithLMHeadModel\", \"XLMWithLMHeadModel\"]:\n",
    "            doc_kwargs[\"mask\"] = \"[MASK]\" if mask is None else mask\n",
    "            code_sample = TF_MASKED_LM_SAMPLE if is_tf_class else PT_MASKED_LM_SAMPLE\n",
    "        elif \"LMHead\" in model_class or \"CausalLM\" in model_class:\n",
    "            code_sample = TF_CAUSAL_LM_SAMPLE if is_tf_class else PT_CAUSAL_LM_SAMPLE\n",
    "        elif \"Model\" in model_class or \"Encoder\" in model_class:\n",
    "            code_sample = TF_BASE_MODEL_SAMPLE if is_tf_class else PT_BASE_MODEL_SAMPLE\n",
    "        else:\n",
    "            raise ValueError(f\"Docstring can't be built for model {model_class}\")\n",
    "\n",
    "        output_doc = _prepare_output_docstrings(output_type, config_class) if output_type is not None else \"\"\n",
    "        built_doc = code_sample.format(**doc_kwargs)\n",
    "        fn.__doc__ = (fn.__doc__ or \"\") + \"\".join(docstr) + output_doc + built_doc\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def replace_return_docstrings(output_type=None, config_class=None):\n",
    "    def docstring_decorator(fn):\n",
    "        docstrings = fn.__doc__\n",
    "        lines = docstrings.split(\"\\n\")\n",
    "        i = 0\n",
    "        while i < len(lines) and re.search(r\"^\\s*Returns?:\\s*$\", lines[i]) is None:\n",
    "            i += 1\n",
    "        if i < len(lines):\n",
    "            lines[i] = _prepare_output_docstrings(output_type, config_class)\n",
    "            docstrings = \"\\n\".join(lines)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"The function {fn} should have an empty 'Return:' or 'Returns:' in its docstring as placeholder, current docstring is:\\n{docstrings}\"\n",
    "            )\n",
    "        fn.__doc__ = docstrings\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def is_remote_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "\n",
    "def hf_bucket_url(\n",
    "    model_id: str, filename: str, subfolder: Optional[str] = None, revision: Optional[str] = None, mirror=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Resolve a model identifier, a file name, and an optional revision id, to a huggingface.co-hosted url, redirecting\n",
    "    to Cloudfront (a Content Delivery Network, or CDN) for large files.\n",
    "\n",
    "    Cloudfront is replicated over the globe so downloads are way faster for the end user (and it also lowers our\n",
    "    bandwidth costs).\n",
    "\n",
    "    Cloudfront aggressively caches files by default (default TTL is 24 hours), however this is not an issue here\n",
    "    because we migrated to a git-based versioning system on huggingface.co, so we now store the files on S3/Cloudfront\n",
    "    in a content-addressable way (i.e., the file name is its hash). Using content-addressable filenames means cache\n",
    "    can't ever be stale.\n",
    "\n",
    "    In terms of client-side caching from this library, we base our caching on the objects' ETag. An object' ETag is:\n",
    "    its sha1 if stored in git, or its sha256 if stored in git-lfs. Files cached locally from transformers before v3.5.0\n",
    "    are not shared with those new files, because the cached file's name contains a hash of the url (which changed).\n",
    "    \"\"\"\n",
    "    if subfolder is not None:\n",
    "        filename = f\"{subfolder}/{filename}\"\n",
    "\n",
    "    if mirror:\n",
    "        endpoint = PRESET_MIRROR_DICT.get(mirror, mirror)\n",
    "        legacy_format = \"/\" not in model_id\n",
    "        if legacy_format:\n",
    "            return f\"{endpoint}/{model_id}-{filename}\"\n",
    "        else:\n",
    "            return f\"{endpoint}/{model_id}/{filename}\"\n",
    "\n",
    "    if revision is None:\n",
    "        revision = \"main\"\n",
    "    return HUGGINGFACE_CO_PREFIX.format(model_id=model_id, revision=revision, filename=filename)\n",
    "\n",
    "\n",
    "def url_to_filename(url: str, etag: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert `url` into a hashed filename in a repeatable way. If `etag` is specified, append its hash to the url's,\n",
    "    delimited by a period. If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name so that TF 2.0 can\n",
    "    identify it as a HDF5 file (see\n",
    "    https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n",
    "    \"\"\"\n",
    "    url_bytes = url.encode(\"utf-8\")\n",
    "    filename = sha256(url_bytes).hexdigest()\n",
    "\n",
    "    if etag:\n",
    "        etag_bytes = etag.encode(\"utf-8\")\n",
    "        filename += \".\" + sha256(etag_bytes).hexdigest()\n",
    "\n",
    "    if url.endswith(\".h5\"):\n",
    "        filename += \".h5\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def filename_to_url(filename, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Return the url and etag (which may be ``None``) stored for `filename`. Raise ``EnvironmentError`` if `filename` or\n",
    "    its stored metadata do not exist.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
    "\n",
    "    meta_path = cache_path + \".json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
    "\n",
    "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    url = metadata[\"url\"]\n",
    "    etag = metadata[\"etag\"]\n",
    "\n",
    "    return url, etag\n",
    "\n",
    "\n",
    "def get_cached_models(cache_dir: Union[str, Path] = None) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Returns a list of tuples representing model binaries that are cached locally. Each tuple has shape\n",
    "    :obj:`(model_url, etag, size_MB)`. Filenames in :obj:`cache_dir` are use to get the metadata for each model, only\n",
    "    urls ending with `.bin` are added.\n",
    "\n",
    "    Args:\n",
    "        cache_dir (:obj:`Union[str, Path]`, `optional`):\n",
    "            The cache directory to search for models within. Will default to the transformers cache if unset.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple]: List of tuples each with shape :obj:`(model_url, etag, size_MB)`\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    elif isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    cached_models = []\n",
    "    for file in os.listdir(cache_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            meta_path = os.path.join(cache_dir, file)\n",
    "            with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
    "                metadata = json.load(meta_file)\n",
    "                url = metadata[\"url\"]\n",
    "                etag = metadata[\"etag\"]\n",
    "                if url.endswith(\".bin\"):\n",
    "                    size_MB = os.path.getsize(meta_path.strip(\".json\")) / 1e6\n",
    "                    cached_models.append((url, etag, size_MB))\n",
    "\n",
    "    return cached_models\n",
    "\n",
    "\n",
    "def cached_path(\n",
    "    url_or_filename,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    resume_download=False,\n",
    "    user_agent: Union[Dict, str, None] = None,\n",
    "    extract_compressed_file=False,\n",
    "    force_extract=False,\n",
    "    use_auth_token: Union[bool, str, None] = None,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path), determine which. If it's a URL, download the file\n",
    "    and cache it, and return the path to the cached file. If it's already a local path, make sure the file exists and\n",
    "    then return the path\n",
    "\n",
    "    Args:\n",
    "        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n",
    "        force_download: if True, re-download the file even if it's already cached in the cache dir.\n",
    "        resume_download: if True, resume the download if incompletely received file is found.\n",
    "        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n",
    "        use_auth_token: Optional string or boolean to use as Bearer token for remote files. If True,\n",
    "            will get token from ~/.huggingface.\n",
    "        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n",
    "            file in a folder along the archive.\n",
    "        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n",
    "            re-extract the archive and override the folder where it was extracted.\n",
    "\n",
    "    Return:\n",
    "        Local path (string) of file or if networking is off, last version of file cached on disk.\n",
    "\n",
    "    Raises:\n",
    "        In case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    if is_remote_url(url_or_filename):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        output_path = get_from_cache(\n",
    "            url_or_filename,\n",
    "            cache_dir=cache_dir,\n",
    "            force_download=force_download,\n",
    "            proxies=proxies,\n",
    "            resume_download=resume_download,\n",
    "            user_agent=user_agent,\n",
    "            use_auth_token=use_auth_token,\n",
    "            local_files_only=local_files_only,\n",
    "        )\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        output_path = url_or_filename\n",
    "    elif urlparse(url_or_filename).scheme == \"\":\n",
    "        # File, but it doesn't exist.\n",
    "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
    "\n",
    "    if extract_compressed_file:\n",
    "        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\n",
    "            return output_path\n",
    "\n",
    "        # Path where we extract compressed archives\n",
    "        # We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"\n",
    "        output_dir, output_file = os.path.split(output_path)\n",
    "        output_extract_dir_name = output_file.replace(\".\", \"-\") + \"-extracted\"\n",
    "        output_path_extracted = os.path.join(output_dir, output_extract_dir_name)\n",
    "\n",
    "        if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:\n",
    "            return output_path_extracted\n",
    "\n",
    "        # Prevent parallel extractions\n",
    "        lock_path = output_path + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            shutil.rmtree(output_path_extracted, ignore_errors=True)\n",
    "            os.makedirs(output_path_extracted)\n",
    "            if is_zipfile(output_path):\n",
    "                with ZipFile(output_path, \"r\") as zip_file:\n",
    "                    zip_file.extractall(output_path_extracted)\n",
    "                    zip_file.close()\n",
    "            elif tarfile.is_tarfile(output_path):\n",
    "                tar_file = tarfile.open(output_path)\n",
    "                tar_file.extractall(output_path_extracted)\n",
    "                tar_file.close()\n",
    "            else:\n",
    "                raise EnvironmentError(\"Archive format of {} could not be identified\".format(output_path))\n",
    "\n",
    "        return output_path_extracted\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n",
    "    \"\"\"\n",
    "    Formats a user-agent string with basic info about a request.\n",
    "    \"\"\"\n",
    "    ua = \"transformers/{}; python/{}\".format(__version__, sys.version.split()[0])\n",
    "    if is_torch_available():\n",
    "        ua += f\"; torch/{_torch_version}\"\n",
    "    if is_tf_available():\n",
    "        ua += f\"; tensorflow/{_tf_version}\"\n",
    "    if isinstance(user_agent, dict):\n",
    "        ua += \"; \" + \"; \".join(\"{}/{}\".format(k, v) for k, v in user_agent.items())\n",
    "    elif isinstance(user_agent, str):\n",
    "        ua += \"; \" + user_agent\n",
    "    return ua\n",
    "\n",
    "\n",
    "def http_get(url: str, temp_file: BinaryIO, proxies=None, resume_size=0, headers: Optional[Dict[str, str]] = None):\n",
    "    \"\"\"\n",
    "    Donwload remote file. Do not gobble up errors.\n",
    "    \"\"\"\n",
    "    headers = copy.deepcopy(headers)\n",
    "    if resume_size > 0:\n",
    "        headers[\"Range\"] = \"bytes=%d-\" % (resume_size,)\n",
    "    r = requests.get(url, stream=True, proxies=proxies, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    content_length = r.headers.get(\"Content-Length\")\n",
    "    total = resume_size + int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        total=total,\n",
    "        initial=resume_size,\n",
    "        desc=\"Downloading\",\n",
    "        disable=bool(logging.get_verbosity() == logging.NOTSET),\n",
    "    )\n",
    "    for chunk in r.iter_content(chunk_size=1024):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def get_from_cache(\n",
    "    url: str,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    etag_timeout=10,\n",
    "    resume_download=False,\n",
    "    user_agent: Union[Dict, str, None] = None,\n",
    "    use_auth_token: Union[bool, str, None] = None,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a URL, look for the corresponding file in the local cache. If it's not there, download it. Then return the\n",
    "    path to the cached file.\n",
    "\n",
    "    Return:\n",
    "        Local path (string) of file or if networking is off, last version of file cached on disk.\n",
    "\n",
    "    Raises:\n",
    "        In case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    headers = {\"user-agent\": http_user_agent(user_agent)}\n",
    "    if isinstance(use_auth_token, str):\n",
    "        headers[\"authorization\"] = \"Bearer {}\".format(use_auth_token)\n",
    "    elif use_auth_token:\n",
    "        token = HfFolder.get_token()\n",
    "        if token is None:\n",
    "            raise EnvironmentError(\"You specified use_auth_token=True, but a huggingface token was not found.\")\n",
    "        headers[\"authorization\"] = \"Bearer {}\".format(token)\n",
    "\n",
    "    url_to_download = url\n",
    "    etag = None\n",
    "    if not local_files_only:\n",
    "        try:\n",
    "            r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
    "            r.raise_for_status()\n",
    "            etag = r.headers.get(\"X-Linked-Etag\") or r.headers.get(\"ETag\")\n",
    "            # We favor a custom header indicating the etag of the linked resource, and\n",
    "            # we fallback to the regular etag header.\n",
    "            # If we don't have any of those, raise an error.\n",
    "            if etag is None:\n",
    "                raise OSError(\n",
    "                    \"Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\"\n",
    "                )\n",
    "            # In case of a redirect,\n",
    "            # save an extra redirect on the request.get call,\n",
    "            # and ensure we download the exact atomic version even if it changed\n",
    "            # between the HEAD and the GET (unlikely, but hey).\n",
    "            if 300 <= r.status_code <= 399:\n",
    "                url_to_download = r.headers[\"Location\"]\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):\n",
    "            # etag is already None\n",
    "            pass\n",
    "\n",
    "    filename = url_to_filename(url, etag)\n",
    "\n",
    "    # get cache path to put the file\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "\n",
    "    # etag is None == we don't have a connection or we passed local_files_only.\n",
    "    # try to get the last downloaded one\n",
    "    if etag is None:\n",
    "        if os.path.exists(cache_path):\n",
    "            return cache_path\n",
    "        else:\n",
    "            matching_files = [\n",
    "                file\n",
    "                for file in fnmatch.filter(os.listdir(cache_dir), filename.split(\".\")[0] + \".*\")\n",
    "                if not file.endswith(\".json\") and not file.endswith(\".lock\")\n",
    "            ]\n",
    "            if len(matching_files) > 0:\n",
    "                return os.path.join(cache_dir, matching_files[-1])\n",
    "            else:\n",
    "                # If files cannot be found and local_files_only=True,\n",
    "                # the models might've been found if local_files_only=False\n",
    "                # Notify the user about that\n",
    "                if local_files_only:\n",
    "                    raise FileNotFoundError(\n",
    "                        \"Cannot find the requested files in the cached path and outgoing traffic has been\"\n",
    "                        \" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\n",
    "                        \" to False.\"\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Connection error, and we cannot find the requested files in the cached path.\"\n",
    "                        \" Please try again or make sure your Internet connection is on.\"\n",
    "                    )\n",
    "\n",
    "    # From now on, etag is not None.\n",
    "    if os.path.exists(cache_path) and not force_download:\n",
    "        return cache_path\n",
    "\n",
    "    # Prevent parallel downloads of the same file with a lock.\n",
    "    lock_path = cache_path + \".lock\"\n",
    "    with FileLock(lock_path):\n",
    "\n",
    "        # If the download just completed while the lock was activated.\n",
    "        if os.path.exists(cache_path) and not force_download:\n",
    "            # Even if returning early like here, the lock will be released.\n",
    "            return cache_path\n",
    "\n",
    "        if resume_download:\n",
    "            incomplete_path = cache_path + \".incomplete\"\n",
    "\n",
    "            @contextmanager\n",
    "            def _resumable_file_manager() -> \"io.BufferedWriter\":\n",
    "                with open(incomplete_path, \"ab\") as f:\n",
    "                    yield f\n",
    "\n",
    "            temp_file_manager = _resumable_file_manager\n",
    "            if os.path.exists(incomplete_path):\n",
    "                resume_size = os.stat(incomplete_path).st_size\n",
    "            else:\n",
    "                resume_size = 0\n",
    "        else:\n",
    "            temp_file_manager = partial(tempfile.NamedTemporaryFile, mode=\"wb\", dir=cache_dir, delete=False)\n",
    "            resume_size = 0\n",
    "\n",
    "        # Download to temporary file, then copy to cache dir once finished.\n",
    "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
    "        with temp_file_manager() as temp_file:\n",
    "            logger.info(\"%s not found in cache or force_download set to True, downloading to %s\", url, temp_file.name)\n",
    "\n",
    "            http_get(url_to_download, temp_file, proxies=proxies, resume_size=resume_size, headers=headers)\n",
    "\n",
    "        logger.info(\"storing %s in cache at %s\", url, cache_path)\n",
    "        os.replace(temp_file.name, cache_path)\n",
    "\n",
    "        logger.info(\"creating metadata file for %s\", cache_path)\n",
    "        meta = {\"url\": url, \"etag\": etag}\n",
    "        meta_path = cache_path + \".json\"\n",
    "        with open(meta_path, \"w\") as meta_file:\n",
    "            json.dump(meta, meta_file)\n",
    "\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "class cached_property(property):\n",
    "    \"\"\"\n",
    "    Descriptor that mimics @property but caches output in member variable.\n",
    "\n",
    "    From tensorflow_datasets\n",
    "\n",
    "    Built-in in functools from Python 3.8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __get__(self, obj, objtype=None):\n",
    "        # See docs.python.org/3/howto/descriptor.html#properties\n",
    "        if obj is None:\n",
    "            return self\n",
    "        if self.fget is None:\n",
    "            raise AttributeError(\"unreadable attribute\")\n",
    "        attr = \"__cached_\" + self.fget.__name__\n",
    "        cached = getattr(obj, attr, None)\n",
    "        if cached is None:\n",
    "            cached = self.fget(obj)\n",
    "            setattr(obj, attr, cached)\n",
    "        return cached\n",
    "\n",
    "\n",
    "def torch_required(func):\n",
    "    # Chose a different decorator name than in tests so it's clear they are not the same.\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if is_torch_available():\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            raise ImportError(f\"Method `{func.__name__}` requires PyTorch.\")\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def tf_required(func):\n",
    "    # Chose a different decorator name than in tests so it's clear they are not the same.\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if is_tf_available():\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            raise ImportError(f\"Method `{func.__name__}` requires TF.\")\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def is_tensor(x):\n",
    "    \"\"\" Tests if ``x`` is a :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`. \"\"\"\n",
    "    if is_torch_available():\n",
    "        import torch\n",
    "\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return True\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if isinstance(x, tf.Tensor):\n",
    "            return True\n",
    "    return isinstance(x, np.ndarray)\n",
    "\n",
    "\n",
    "class ModelOutput(OrderedDict):\n",
    "    \"\"\"\n",
    "    Base class for all model outputs as dataclass. Has a ``__getitem__`` that allows indexing by integer or slice (like\n",
    "    a tuple) or strings (like a dictionary) that will ignore the ``None`` attributes. Otherwise behaves like a regular\n",
    "    python dictionary.\n",
    "\n",
    "    .. warning::\n",
    "        You can't unpack a :obj:`ModelOutput` directly. Use the :meth:`~transformers.file_utils.ModelOutput.to_tuple`\n",
    "        method to convert it to a tuple before.\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        class_fields = fields(self)\n",
    "\n",
    "        # Safety and consistency checks\n",
    "        assert len(class_fields), f\"{self.__class__.__name__} has no fields.\"\n",
    "        assert all(\n",
    "            field.default is None for field in class_fields[1:]\n",
    "        ), f\"{self.__class__.__name__} should not have more than one required field.\"\n",
    "\n",
    "        first_field = getattr(self, class_fields[0].name)\n",
    "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
    "\n",
    "        if other_fields_are_none and not is_tensor(first_field):\n",
    "            try:\n",
    "                iterator = iter(first_field)\n",
    "                first_field_iterator = True\n",
    "            except TypeError:\n",
    "                first_field_iterator = False\n",
    "\n",
    "            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n",
    "            # set the associated fields\n",
    "            if first_field_iterator:\n",
    "                for element in iterator:\n",
    "                    if (\n",
    "                        not isinstance(element, (list, tuple))\n",
    "                        or not len(element) == 2\n",
    "                        or not isinstance(element[0], str)\n",
    "                    ):\n",
    "                        break\n",
    "                    setattr(self, element[0], element[1])\n",
    "                    if element[1] is not None:\n",
    "                        self[element[0]] = element[1]\n",
    "            elif first_field is not None:\n",
    "                self[class_fields[0].name] = first_field\n",
    "        else:\n",
    "            for field in class_fields:\n",
    "                v = getattr(self, field.name)\n",
    "                if v is not None:\n",
    "                    self[field.name] = v\n",
    "\n",
    "    def __delitem__(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def setdefault(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def pop(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if isinstance(k, str):\n",
    "            inner_dict = {k: v for (k, v) in self.items()}\n",
    "            return inner_dict[k]\n",
    "        else:\n",
    "            return self.to_tuple()[k]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name in self.keys() and value is not None:\n",
    "            # Don't call self.__setitem__ to avoid recursion errors\n",
    "            super().__setitem__(name, value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        # Will raise a KeyException if needed\n",
    "        super().__setitem__(key, value)\n",
    "        # Don't call self.__setattr__ to avoid recursion errors\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        \"\"\"\n",
    "        Convert self to a tuple containing all the attributes/keys that are not ``None``.\n",
    "        \"\"\"\n",
    "        return tuple(self[k] for k in self.keys())\n",
    "\n",
    "\n",
    "class _BaseLazyModule(ModuleType):\n",
    "    \"\"\"\n",
    "    Module class that surfaces all objects but only performs associated imports when the objects are requested.\n",
    "    \"\"\"\n",
    "\n",
    "    # Very heavily inspired by optuna.integration._IntegrationModule\n",
    "    # https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py\n",
    "    def __init__(self, name, import_structure):\n",
    "        super().__init__(name)\n",
    "        self._modules = set(import_structure.keys())\n",
    "        self._class_to_module = {}\n",
    "        for key, values in import_structure.items():\n",
    "            for value in values:\n",
    "                self._class_to_module[value] = key\n",
    "        # Needed for autocompletion in an IDE\n",
    "        self.__all__ = list(import_structure.keys()) + sum(import_structure.values(), [])\n",
    "\n",
    "    # Needed for autocompletion in an IDE\n",
    "    def __dir__(self):\n",
    "        return super().__dir__() + self.__all__\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        if name in self._modules:\n",
    "            value = self._get_module(name)\n",
    "        elif name in self._class_to_module.keys():\n",
    "            module = self._get_module(self._class_to_module[name])\n",
    "            value = getattr(module, name)\n",
    "        else:\n",
    "            raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n",
    "\n",
    "        setattr(self, name, value)\n",
    "        return value\n",
    "\n",
    "    def _get_module(self, module_name: str) -> ModuleType:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configure util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.164397Z",
     "start_time": "2021-02-10T06:54:02.137476Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Configuration base class and utilities.\"\"\"\n",
    "\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "# from . import __version__\n",
    "# from .file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class PretrainedConfig(object):\n",
    "    r\"\"\"\n",
    "    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n",
    "    methods for loading/downloading/saving configurations.\n",
    "\n",
    "    Note: A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to\n",
    "    initialize a model does **not** load the model weights. It only affects the model's configuration.\n",
    "\n",
    "    Class attributes (overridden by derived classes)\n",
    "\n",
    "        - **model_type** (:obj:`str`): An identifier for the model type, serialized into the JSON file, and used to\n",
    "          recreate the correct object in :class:`~transformers.AutoConfig`.\n",
    "        - **is_composition** (:obj:`bool`): Whether the config class is composed of multiple sub-configs. In this case\n",
    "          the config has to be initialized from two or more configs of type :class:`~transformers.PretrainedConfig`\n",
    "          like: :class:`~transformers.EncoderDecoderConfig` or :class:`~RagConfig`.\n",
    "        - **keys_to_ignore_at_inference** (:obj:`List[str]`): A list of keys to ignore by default when looking at\n",
    "          dictionary outputs of the model during inference.\n",
    "\n",
    "    Args:\n",
    "        name_or_path (:obj:`str`, `optional`, defaults to :obj:`\"\"`):\n",
    "            Store the string that was passed to :func:`~transformers.PreTrainedModel.from_pretrained` or\n",
    "            :func:`~transformers.TFPreTrainedModel.from_pretrained` as ``pretrained_model_name_or_path`` if the\n",
    "            configuration was created with such a method.\n",
    "        output_hidden_states (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not the model should return all hidden-states.\n",
    "        output_attentions (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not the model should returns all attentions.\n",
    "        return_dict (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not the model should return a :class:`~transformers.file_utils.ModelOutput` instead of a plain\n",
    "            tuple.\n",
    "        is_encoder_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether the model is used as an encoder/decoder or not.\n",
    "        is_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether the model is used as decoder or not (in which case it's used as an encoder).\n",
    "        add_cross_attention (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n",
    "            that can be used as decoder models within the `:class:~transformers.EncoderDecoderModel` class, which\n",
    "            consists of all models in ``AUTO_MODELS_FOR_CAUSAL_LM``.\n",
    "        tie_encoder_decoder (:obj:`bool`, `optional`, defaults to :obj:`False`)\n",
    "            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n",
    "            and decoder model to have the exact same parameter names.\n",
    "        prune_heads (:obj:`Dict[int, List[int]]`, `optional`, defaults to :obj:`{}`):\n",
    "            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n",
    "            heads to prune in said layer.\n",
    "\n",
    "            For instance ``{1: [0, 2], 2: [2, 3]}`` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
    "        xla_device (:obj:`bool`, `optional`):\n",
    "            A flag to indicate if TPU are available or not.\n",
    "        chunk_size_feed_forward (:obj:`int`, `optional`, defaults to :obj:`0`):\n",
    "            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of :obj:`0` means\n",
    "            that the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes\n",
    "            :obj:`n` < sequence_length embeddings at a time. For more information on feed forward chunking, see `How\n",
    "            does Feed Forward Chunking work? <../glossary.html#feed-forward-chunking>`__ .\n",
    "\n",
    "    Parameters for sequence generation\n",
    "\n",
    "        - **max_length** (:obj:`int`, `optional`, defaults to 20) -- Maximum length that will be used by default in the\n",
    "          :obj:`generate` method of the model.\n",
    "        - **min_length** (:obj:`int`, `optional`, defaults to 10) -- Minimum length that will be used by default in the\n",
    "          :obj:`generate` method of the model.\n",
    "        - **do_sample** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Flag that will be used by default in the\n",
    "          :obj:`generate` method of the model. Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "        - **early_stopping** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Flag that will be used by default\n",
    "          in the :obj:`generate` method of the model. Whether to stop the beam search when at least ``num_beams``\n",
    "          sentences are finished per batch or not.\n",
    "        - **num_beams** (:obj:`int`, `optional`, defaults to 1) -- Number of beams for beam search that will be used by\n",
    "          default in the :obj:`generate` method of the model. 1 means no beam search.\n",
    "        - **num_beam_groups** (:obj:`int`, `optional`, defaults to 1) -- Number of groups to divide :obj:`num_beams`\n",
    "          into in order to ensure diversity among different groups of beams that will be used by default in the\n",
    "          :obj:`generate` method of the model. 1 means no group beam search.\n",
    "        - **diversity_penalty** (:obj:`float`, `optional`, defaults to 0.0) -- Value to control diversity for group\n",
    "          beam search. that will be used by default in the :obj:`generate` method of the model. 0 means no diversity\n",
    "          penalty. The higher the penalty, the more diverse are the outputs.\n",
    "        - **temperature** (:obj:`float`, `optional`, defaults to 1) -- The value used to module the next token\n",
    "          probabilities that will be used by default in the :obj:`generate` method of the model. Must be strictly\n",
    "          positive.\n",
    "        - **top_k** (:obj:`int`, `optional`, defaults to 50) -- Number of highest probability vocabulary tokens to keep\n",
    "          for top-k-filtering that will be used by default in the :obj:`generate` method of the model.\n",
    "        - **top_p** (:obj:`float`, `optional`, defaults to 1) -- Value that will be used by default in the\n",
    "          :obj:`generate` method of the model for ``top_p``. If set to float < 1, only the most probable tokens with\n",
    "          probabilities that add up to ``top_p`` or higher are kept for generation.\n",
    "        - **repetition_penalty** (:obj:`float`, `optional`, defaults to 1) -- Parameter for repetition penalty that\n",
    "          will be used by default in the :obj:`generate` method of the model. 1.0 means no penalty.\n",
    "        - **length_penalty** (:obj:`float`, `optional`, defaults to 1) -- Exponential penalty to the length that will\n",
    "          be used by default in the :obj:`generate` method of the model.\n",
    "        - **no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) -- Value that will be used by default in the\n",
    "          :obj:`generate` method of the model for ``no_repeat_ngram_size``. If set to int > 0, all ngrams of that size\n",
    "          can only occur once.\n",
    "        - **encoder_no_repeat_ngram_size** (:obj:`int`, `optional`, defaults to 0) -- Value that will be used by\n",
    "          default in the :obj:`generate` method of the model for ``encoder_no_repeat_ngram_size``. If set to int > 0,\n",
    "          all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the ``decoder_input_ids``.\n",
    "        - **bad_words_ids** (:obj:`List[int]`, `optional`) -- List of token ids that are not allowed to be generated\n",
    "          that will be used by default in the :obj:`generate` method of the model. In order to get the tokens of the\n",
    "          words that should not appear in the generated text, use :obj:`tokenizer.encode(bad_word,\n",
    "          add_prefix_space=True)`.\n",
    "        - **num_return_sequences** (:obj:`int`, `optional`, defaults to 1) -- Number of independently computed returned\n",
    "          sequences for each element in the batch that will be used by default in the :obj:`generate` method of the\n",
    "          model.\n",
    "        - **output_scores** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether the model should return the\n",
    "          logits when used for generation\n",
    "        - **return_dict_in_generate** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether the model should\n",
    "          return a :class:`~transformers.file_utils.ModelOutput` instead of a :obj:`torch.LongTensor`\n",
    "\n",
    "\n",
    "    Parameters for fine-tuning tasks\n",
    "\n",
    "        - **architectures** (:obj:`List[str]`, `optional`) -- Model architectures that can be used with the model\n",
    "          pretrained weights.\n",
    "        - **finetuning_task** (:obj:`str`, `optional`) -- Name of the task used to fine-tune the model. This can be\n",
    "          used when converting from an original (TensorFlow or PyTorch) checkpoint.\n",
    "        - **id2label** (:obj:`Dict[int, str]`, `optional`) -- A map from index (for instance prediction index, or\n",
    "          target index) to label.\n",
    "        - **label2id** (:obj:`Dict[str, int]`, `optional`) -- A map from label to index for the model.\n",
    "        - **num_labels** (:obj:`int`, `optional`) -- Number of labels to use in the last layer added to the model,\n",
    "          typically for a classification task.\n",
    "        - **task_specific_params** (:obj:`Dict[str, Any]`, `optional`) -- Additional keyword arguments to store for the\n",
    "          current task.\n",
    "\n",
    "    Parameters linked to the tokenizer\n",
    "\n",
    "        - **tokenizer_class** (:obj:`str`, `optional`) -- The name of the associated tokenizer class to use (if none is\n",
    "          set, will use the tokenizer associated to the model by default).\n",
    "        - **prefix** (:obj:`str`, `optional`) -- A specific prompt that should be added at the beginning of each text\n",
    "          before calling the model.\n",
    "        - **bos_token_id** (:obj:`int`, `optional`)) -- The id of the `beginning-of-stream` token.\n",
    "        - **pad_token_id** (:obj:`int`, `optional`)) -- The id of the `padding` token.\n",
    "        - **eos_token_id** (:obj:`int`, `optional`)) -- The id of the `end-of-stream` token.\n",
    "        - **decoder_start_token_id** (:obj:`int`, `optional`)) -- If an encoder-decoder model starts decoding with a\n",
    "          different token than `bos`, the id of that token.\n",
    "        - **sep_token_id** (:obj:`int`, `optional`)) -- The id of the `separation` token.\n",
    "\n",
    "    PyTorch specific parameters\n",
    "\n",
    "        - **torchscript** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether or not the model should be\n",
    "          used with Torchscript.\n",
    "        - **tie_word_embeddings** (:obj:`bool`, `optional`, defaults to :obj:`True`) -- Whether the model's input and\n",
    "          output word embeddings should be tied. Note that this is only relevant if the model has a output word\n",
    "          embedding layer.\n",
    "\n",
    "    TensorFlow specific parameters\n",
    "\n",
    "        - **use_bfloat16** (:obj:`bool`, `optional`, defaults to :obj:`False`) -- Whether or not the model should use\n",
    "          BFloat16 scalars (only used by some TensorFlow models).\n",
    "    \"\"\"\n",
    "    model_type: str = \"\"\n",
    "    is_composition: bool = False\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Attributes with defaults\n",
    "        self.return_dict = kwargs.pop(\"return_dict\", True)\n",
    "        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n",
    "        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n",
    "        self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n",
    "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
    "        self.pruned_heads = kwargs.pop(\"pruned_heads\", {})\n",
    "        self.tie_word_embeddings = kwargs.pop(\n",
    "            \"tie_word_embeddings\", True\n",
    "        )  # Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.\n",
    "\n",
    "        # Is decoder is used in encoder-decoder models to differentiate encoder from decoder\n",
    "        self.is_encoder_decoder = kwargs.pop(\"is_encoder_decoder\", False)\n",
    "        self.is_decoder = kwargs.pop(\"is_decoder\", False)\n",
    "        self.add_cross_attention = kwargs.pop(\"add_cross_attention\", False)\n",
    "        self.tie_encoder_decoder = kwargs.pop(\"tie_encoder_decoder\", False)\n",
    "\n",
    "        # Parameters for sequence generation\n",
    "        self.max_length = kwargs.pop(\"max_length\", 20)\n",
    "        self.min_length = kwargs.pop(\"min_length\", 0)\n",
    "        self.do_sample = kwargs.pop(\"do_sample\", False)\n",
    "        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n",
    "        self.num_beams = kwargs.pop(\"num_beams\", 1)\n",
    "        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n",
    "        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.top_k = kwargs.pop(\"top_k\", 50)\n",
    "        self.top_p = kwargs.pop(\"top_p\", 1.0)\n",
    "        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n",
    "        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n",
    "        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n",
    "        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n",
    "        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n",
    "        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n",
    "        self.chunk_size_feed_forward = kwargs.pop(\"chunk_size_feed_forward\", 0)\n",
    "        self.output_scores = kwargs.pop(\"output_scores\", False)\n",
    "        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n",
    "\n",
    "        # Fine-tuning task arguments\n",
    "        self.architectures = kwargs.pop(\"architectures\", None)\n",
    "        self.finetuning_task = kwargs.pop(\"finetuning_task\", None)\n",
    "        self.id2label = kwargs.pop(\"id2label\", None)\n",
    "        self.label2id = kwargs.pop(\"label2id\", None)\n",
    "        if self.id2label is not None:\n",
    "            kwargs.pop(\"num_labels\", None)\n",
    "            self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n",
    "            # Keys are always strings in JSON so convert ids to int here.\n",
    "        else:\n",
    "            self.num_labels = kwargs.pop(\"num_labels\", 2)\n",
    "\n",
    "        # Tokenizer arguments TODO: eventually tokenizer and models should share the same config\n",
    "        self.tokenizer_class = kwargs.pop(\"tokenizer_class\", None)\n",
    "        self.prefix = kwargs.pop(\"prefix\", None)\n",
    "        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n",
    "        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n",
    "        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n",
    "        self.sep_token_id = kwargs.pop(\"sep_token_id\", None)\n",
    "\n",
    "        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n",
    "\n",
    "        # task specific arguments\n",
    "        self.task_specific_params = kwargs.pop(\"task_specific_params\", None)\n",
    "\n",
    "        # TPU arguments\n",
    "        self.xla_device = kwargs.pop(\"xla_device\", None)\n",
    "\n",
    "        # Name or path to the pretrained checkpoint\n",
    "        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n",
    "\n",
    "        # Drop the transformers version info\n",
    "        kwargs.pop(\"transformers_version\", None)\n",
    "\n",
    "        # Additional attributes without default values\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                setattr(self, key, value)\n",
    "            except AttributeError as err:\n",
    "                logger.error(\"Can't set {} with value {} for {}\".format(key, value, self))\n",
    "                raise err\n",
    "\n",
    "    @property\n",
    "    def name_or_path(self) -> str:\n",
    "        return self._name_or_path\n",
    "\n",
    "    @name_or_path.setter\n",
    "    def name_or_path(self, value):\n",
    "        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n",
    "\n",
    "    @property\n",
    "    def use_return_dict(self) -> bool:\n",
    "        \"\"\"\n",
    "        :obj:`bool`: Whether or not return :class:`~transformers.file_utils.ModelOutput` instead of tuples.\n",
    "        \"\"\"\n",
    "        # If torchscript is set, force `return_dict=False` to avoid jit errors\n",
    "        return self.return_dict and not self.torchscript\n",
    "\n",
    "    @property\n",
    "    def num_labels(self) -> int:\n",
    "        \"\"\"\n",
    "        :obj:`int`: The number of labels for classification models.\n",
    "        \"\"\"\n",
    "        return len(self.id2label)\n",
    "\n",
    "    @num_labels.setter\n",
    "    def num_labels(self, num_labels: int):\n",
    "        self.id2label = {i: \"LABEL_{}\".format(i) for i in range(num_labels)}\n",
    "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
    "        \"\"\"\n",
    "        Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the\n",
    "        :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n",
    "\n",
    "        Args:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            raise AssertionError(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n",
    "\n",
    "        self.to_json_file(output_config_file, use_diff=True)\n",
    "        logger.info(\"Configuration saved in {}\".format(output_config_file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n",
    "        r\"\"\"\n",
    "        Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pretrained model\n",
    "        configuration.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                This can be either:\n",
    "\n",
    "                - a string, the `model id` of a pretrained model configuration hosted inside a model repo on\n",
    "                  huggingface.co. Valid model ids can be located at the root-level, like ``bert-base-uncased``, or\n",
    "                  namespaced under a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                - a path to a `directory` containing a configuration file saved using the\n",
    "                  :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g., ``./my_model_directory/``.\n",
    "                - a path or url to a saved configuration JSON `file`, e.g.,\n",
    "                  ``./my_model_directory/configuration.json``.\n",
    "            cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
    "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
    "                standard cache should not be used.\n",
    "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to force to (re-)download the configuration files and override the cached versions if\n",
    "                they exist.\n",
    "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to delete incompletely received file. Attempts to resume the download if such a file\n",
    "                exists.\n",
    "            proxies (:obj:`Dict[str, str]`, `optional`):\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
    "                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
    "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
    "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
    "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
    "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
    "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
    "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
    "                identifier allowed by git.\n",
    "            return_unused_kwargs (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If :obj:`False`, then this function returns just the final configuration object.\n",
    "\n",
    "                If :obj:`True`, then this functions returns a :obj:`Tuple(config, unused_kwargs)` where `unused_kwargs`\n",
    "                is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,\n",
    "                the part of ``kwargs`` which has not been used to update ``config`` and is otherwise ignored.\n",
    "            kwargs (:obj:`Dict[str, Any]`, `optional`):\n",
    "                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n",
    "                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n",
    "                by the ``return_unused_kwargs`` keyword parameter.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from this pretrained model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n",
    "            # derived class: BertConfig\n",
    "            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from huggingface.co and cache.\n",
    "            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
    "            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n",
    "            config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False)\n",
    "            assert config.output_attentions == True\n",
    "            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True,\n",
    "                                                               foo=False, return_unused_kwargs=True)\n",
    "            assert config.output_attentions == True\n",
    "            assert unused_kwargs == {'foo': False}\n",
    "\n",
    "        \"\"\"\n",
    "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
    "        return cls.from_dict(config_dict, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_config_dict(\n",
    "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
    "    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a\n",
    "        :class:`~transformers.PretrainedConfig` using ``from_dict``.\n",
    "\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n",
    "\n",
    "        \"\"\"\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "\n",
    "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "        if os.path.isdir(pretrained_model_name_or_path):\n",
    "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
    "        elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "            config_file = pretrained_model_name_or_path\n",
    "        else:\n",
    "            config_file = hf_bucket_url(\n",
    "                pretrained_model_name_or_path, filename=CONFIG_NAME, revision=revision, mirror=None\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Load from URL or cache if already cached\n",
    "            resolved_config_file = cached_path(\n",
    "                config_file,\n",
    "                cache_dir=cache_dir,\n",
    "                force_download=force_download,\n",
    "                proxies=proxies,\n",
    "                resume_download=resume_download,\n",
    "                local_files_only=local_files_only,\n",
    "                use_auth_token=use_auth_token,\n",
    "            )\n",
    "            # Load config dict\n",
    "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
    "\n",
    "        except EnvironmentError as err:\n",
    "            logger.error(err)\n",
    "            msg = (\n",
    "                f\"Can't load config for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            msg = (\n",
    "                \"Couldn't reach server at '{}' to download configuration file or \"\n",
    "                \"configuration file is not a valid JSON file. \"\n",
    "                \"Please check network or file content here: {}.\".format(config_file, resolved_config_file)\n",
    "            )\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        if resolved_config_file == config_file:\n",
    "            logger.info(\"loading configuration file {}\".format(config_file))\n",
    "        else:\n",
    "            logger.info(\"loading configuration file {} from cache at {}\".format(config_file, resolved_config_file))\n",
    "\n",
    "        return config_dict, kwargs\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n",
    "        \"\"\"\n",
    "        Instantiates a :class:`~transformers.PretrainedConfig` from a Python dictionary of parameters.\n",
    "\n",
    "        Args:\n",
    "            config_dict (:obj:`Dict[str, Any]`):\n",
    "                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n",
    "                retrieved from a pretrained checkpoint by leveraging the\n",
    "                :func:`~transformers.PretrainedConfig.get_config_dict` method.\n",
    "            kwargs (:obj:`Dict[str, Any]`):\n",
    "                Additional parameters from which to initialize the configuration object.\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from those parameters.\n",
    "        \"\"\"\n",
    "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
    "\n",
    "        config = cls(**config_dict)\n",
    "\n",
    "        if hasattr(config, \"pruned_heads\"):\n",
    "            config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items())\n",
    "\n",
    "        # Update config with kwargs if needed\n",
    "        to_remove = []\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "                to_remove.append(key)\n",
    "        for key in to_remove:\n",
    "            kwargs.pop(key, None)\n",
    "\n",
    "        logger.info(\"Model config %s\", str(config))\n",
    "        if return_unused_kwargs:\n",
    "            return config, kwargs\n",
    "        else:\n",
    "            return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"PretrainedConfig\":\n",
    "        \"\"\"\n",
    "        Instantiates a :class:`~transformers.PretrainedConfig` from the path to a JSON file of parameters.\n",
    "\n",
    "        Args:\n",
    "            json_file (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Path to the JSON file containing the parameters.\n",
    "\n",
    "        Returns:\n",
    "            :class:`PretrainedConfig`: The configuration object instantiated from that JSON file.\n",
    "\n",
    "        \"\"\"\n",
    "        config_dict = cls._dict_from_json_file(json_file)\n",
    "        return cls(**config_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "            text = reader.read()\n",
    "        return json.loads(text)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{} {}\".format(self.__class__.__name__, self.to_json_string())\n",
    "\n",
    "    def to_diff_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Removes all attributes from config which correspond to the default config attributes for better readability and\n",
    "        serializes to a Python dictionary.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n",
    "        \"\"\"\n",
    "        config_dict = self.to_dict()\n",
    "\n",
    "        # get the default config dict\n",
    "        default_config_dict = PretrainedConfig().to_dict()\n",
    "\n",
    "        # get class specific config dict\n",
    "        class_config_dict = self.__class__().to_dict() if not self.is_composition else {}\n",
    "\n",
    "        serializable_config_dict = {}\n",
    "\n",
    "        # only serialize values that differ from the default config\n",
    "        for key, value in config_dict.items():\n",
    "            if (\n",
    "                key not in default_config_dict\n",
    "                or key == \"transformers_version\"\n",
    "                or value != default_config_dict[key]\n",
    "                or (key in class_config_dict and value != class_config_dict[key])\n",
    "            ):\n",
    "                serializable_config_dict[key] = value\n",
    "\n",
    "        return serializable_config_dict\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serializes this instance to a Python dictionary.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n",
    "        \"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        if hasattr(self.__class__, \"model_type\"):\n",
    "            output[\"model_type\"] = self.__class__.model_type\n",
    "\n",
    "        # Transformers version when serializing the model\n",
    "        output[\"transformers_version\"] = __version__\n",
    "\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self, use_diff: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Serializes this instance to a JSON string.\n",
    "\n",
    "        Args:\n",
    "            use_diff (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, only the difference between the config instance and the default\n",
    "                ``PretrainedConfig()`` is serialized to JSON string.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`str`: String containing all the attributes that make up this configuration instance in JSON format.\n",
    "        \"\"\"\n",
    "        if use_diff is True:\n",
    "            config_dict = self.to_diff_dict()\n",
    "        else:\n",
    "            config_dict = self.to_dict()\n",
    "        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n",
    "        \"\"\"\n",
    "        Save this instance to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            json_file_path (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Path to the JSON file in which this configuration instance's parameters will be saved.\n",
    "            use_diff (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, only the difference between the config instance and the default\n",
    "                ``PretrainedConfig()`` is serialized to JSON file.\n",
    "        \"\"\"\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(self.to_json_string(use_diff=use_diff))\n",
    "\n",
    "    def update(self, config_dict: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Updates attributes of this class with attributes from ``config_dict``.\n",
    "\n",
    "        Args:\n",
    "            config_dict (:obj:`Dict[str, Any]`): Dictionary of attributes that shall be updated for this class.\n",
    "        \"\"\"\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.171531Z",
     "start_time": "2021-02-10T06:54:02.165849Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" BERT model configuration \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/config.json\",\n",
    "    \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\",\n",
    "    \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/config.json\",\n",
    "    \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese\": \"https://huggingface.co/cl-tohoku/bert-base-japanese/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/config.json\",\n",
    "    \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/config.json\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "}\n",
    "\n",
    "\n",
    "class BertConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a :class:`~transformers.BertModel` or a\n",
    "    :class:`~transformers.TFBertModel`. It is used to instantiate a BERT model according to the specified arguments,\n",
    "    defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration\n",
    "    to that of the BERT `bert-base-uncased <https://huggingface.co/bert-base-uncased>`__ architecture.\n",
    "\n",
    "    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n",
    "    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
    "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
    "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of hidden layers in the Transformer encoder.\n",
    "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
    "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
    "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
    "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
    "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
    "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
    "            just in case (e.g., 512 or 1024 or 2048).\n",
    "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
    "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "        position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
    "            Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
    "            :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
    "            :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
    "            <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
    "            `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
    "            <https://arxiv.org/abs/2009.13658>`__.\n",
    "        use_cache (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if ``config.is_decoder=True``.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> from transformers import BertModel, BertConfig\n",
    "\n",
    "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
    "        >>> configuration = BertConfig()\n",
    "\n",
    "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
    "        >>> model = BertModel(configuration)\n",
    "\n",
    "        >>> # Accessing the model configuration\n",
    "        >>> configuration = model.config\n",
    "    \"\"\"\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        gradient_checkpointing=False,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.554117Z",
     "start_time": "2021-02-10T06:54:02.172966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def _gelu_python(x):\n",
    "    \"\"\"\n",
    "    Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n",
    "    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +\n",
    "    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in\n",
    "    torch.nn.functional Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def gelu_new(x):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.4\"):\n",
    "    gelu = _gelu_python\n",
    "else:\n",
    "    gelu = F.gelu\n",
    "\n",
    "\n",
    "def gelu_fast(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(x * 0.7978845608 * (1.0 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def _silu_python(x):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learning (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.7\"):\n",
    "    silu = _silu_python\n",
    "else:\n",
    "    silu = F.silu\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "def linear_act(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"silu\": silu,\n",
    "    \"swish\": silu,\n",
    "    \"gelu\": gelu,\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"gelu_new\": gelu_new,\n",
    "    \"gelu_fast\": gelu_fast,\n",
    "    \"mish\": mish,\n",
    "    \"linear\": linear_act,\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "}\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "    if activation_string in ACT2FN:\n",
    "        return ACT2FN[activation_string]\n",
    "    else:\n",
    "        raise KeyError(\"function {} not found in ACT2FN mapping {}\".format(activation_string, list(ACT2FN.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.590471Z",
     "start_time": "2021-02-10T06:54:02.556895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# from .file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs, with potential hidden states and attentions.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPooling(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that also contains a pooling of the last hidden states.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n",
    "            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n",
    "            prediction (classification) objective during pretraining.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs, with potential hidden states and attentions.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that also contains a pooling of the last hidden states.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n",
    "            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n",
    "            prediction (classification) objective during pretraining.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
    "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
    "            encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
    "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n",
    "    decoding.\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
    "\n",
    "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
    "            1, hidden_size)` is output.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tupel(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithCrossAttentions(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
    "            cross-attention heads.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` tuples of length :obj:`config.n_layers`, with each tuple containing the\n",
    "            cached key, value states of the self-attention and the cross-attention layers if model is used in\n",
    "            encoder-decoder setting. Only relevant if ``config.is_decoder = True``.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequenceClassifierOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tupel(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
    "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MaskedLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for masked language models outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Masked language modeling (MLM) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqLMOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for sequence-to-sequence language models outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Language modeling loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NextSentencePredictorOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of models predicting if two sentences are consecutive or not.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`next_sentence_label` is provided):\n",
    "            Next sequence prediction (classification) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequenceClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqSequenceClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sequence-to-sequence sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultipleChoiceModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of multiple choice models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Classification loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices)`):\n",
    "            `num_choices` is the second dimension of the input tensors. (see `input_ids` above).\n",
    "\n",
    "            Classification scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenClassifierOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of token classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when ``labels`` is provided) :\n",
    "            Classification loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.num_labels)`):\n",
    "            Classification scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnsweringModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of question answering models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
    "        start_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-start scores (before SoftMax).\n",
    "        end_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-end scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqQuestionAnsweringModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sequence-to-sequence question answering models.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n",
    "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
    "        start_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-start scores (before SoftMax).\n",
    "        end_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Span-end scores (before SoftMax).\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
    "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
    "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "            shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential decoding.\n",
    "        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
    "        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
    "            weighted average in the cross-attention heads.\n",
    "        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
    "            self-attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_logits: torch.FloatTensor = None\n",
    "    end_logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate logits  process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.650633Z",
     "start_time": "2021-02-10T06:54:02.592322Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import inspect\n",
    "import math\n",
    "from abc import ABC\n",
    "from typing import Callable, Iterable, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from .file_utils import add_start_docstrings\n",
    "\n",
    "\n",
    "LOGITS_PROCESSOR_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`):\n",
    "            Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
    "            or scores for each vocabulary token after SoftMax.\n",
    "        kwargs:\n",
    "            Additional logits processor specific kwargs.\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.vocab_size)`: The processed prediction scores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LogitsProcessor(ABC):\n",
    "    \"\"\"Abstract base class for all logit processors that can be applied during generation.\"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Torch method for processing logits.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LogitsWarper(ABC):\n",
    "    \"\"\"Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Torch method for warping logits.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class LogitsProcessorList(list):\n",
    "    \"\"\"\n",
    "    This class can be used to create a list of :class:`~transformers.LogitsProcessor` or\n",
    "    :class:`~transformers.LogitsWarper` to subsequently process a :obj:`scores` input tensor. This class inherits from\n",
    "    list and adds a specific `__call__` method to apply each :class:`~transformers.LogitsProcessor` or\n",
    "    :class:`~transformers.LogitsProcessor` to the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        for processor in self:\n",
    "            function_args = inspect.signature(processor.__call__).parameters\n",
    "            if len(function_args) > 2:\n",
    "                assert all(\n",
    "                    arg in kwargs for arg in list(function_args.keys())[2:]\n",
    "                ), f\"Make sure that all the required parameters: {list(function_args.keys())} for {processor.__class__} are passed to the logits processor.\"\n",
    "                scores = processor(input_ids, scores, **kwargs)\n",
    "            else:\n",
    "                scores = processor(input_ids, scores)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class MinLengthLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.\n",
    "\n",
    "    Args:\n",
    "        min_length (:obj:`int`):\n",
    "            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(\"Inf\")`.\n",
    "        eos_token_id (:obj:`int`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_length: int, eos_token_id: int):\n",
    "        if not isinstance(min_length, int) or min_length < 0:\n",
    "            raise ValueError(f\"`min_length` has to be a positive integer, but is {min_length}\")\n",
    "\n",
    "        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n",
    "            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n",
    "\n",
    "        self.min_length = min_length\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        if cur_len < self.min_length:\n",
    "            scores[:, self.eos_token_id] = -float(\"inf\")\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TemperatureLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsWarper` for temperature (exponential scaling output probability distribution).\n",
    "\n",
    "    Args:\n",
    "        temperature (:obj:`float`):\n",
    "            The value used to module the logits distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature: float):\n",
    "        if not isinstance(temperature, float) or not (temperature > 0):\n",
    "            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores = scores / self.temperature\n",
    "        return scores\n",
    "\n",
    "\n",
    "class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` enforcing an exponential penalty on repeated sequences.\n",
    "\n",
    "    Args:\n",
    "        repetition_penalty (:obj:`float`):\n",
    "            The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "            <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty: float):\n",
    "        if not isinstance(penalty, float) or not (penalty > 0):\n",
    "            raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n",
    "\n",
    "        self.penalty = penalty\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        score = torch.gather(scores, 1, input_ids)\n",
    "\n",
    "        # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability\n",
    "        score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
    "\n",
    "        scores.scatter_(1, input_ids, score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TopPLogitsWarper(LogitsWarper):\n",
    "    \"\"\"\n",
    "    :class:`transformers.LogitsWarper` that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=\n",
    "    prob_cut_off.\n",
    "\n",
    "    Args:\n",
    "        top_p (:obj:`float`):\n",
    "            If set to < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or higher are\n",
    "            kept for generation.\n",
    "        filter_value (:obj:`float`, `optional`, defaults to :obj:`-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_p, float) or (top_p < 0 or top_p > 1.0):\n",
    "            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
    "\n",
    "        self.top_p = top_p\n",
    "        self.filter_value = filter_value\n",
    "        self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n",
    "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > self.top_p\n",
    "        if self.min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., : self.min_tokens_to_keep - 1] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class TopKLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsWarper` that performs top-k, i.e. restricting to the k highest probability elements.\n",
    "\n",
    "    Args:\n",
    "        top_k (:obj:`int`):\n",
    "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        filter_value (:obj:`float`, `optional`, defaults to :obj:`-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_k, int) or top_k <= 0:\n",
    "            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n",
    "\n",
    "        self.top_k = top_k\n",
    "        self.filter_value = filter_value\n",
    "        self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        top_k = min(max(self.top_k, self.min_tokens_to_keep), scores.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_ngram_tokens(\n",
    "    ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int\n",
    ") -> List[Iterable[int]]:\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "    return banned_tokens\n",
    "\n",
    "\n",
    "class NoRepeatNGramLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces no repetition of n-grams. See `Fairseq\n",
    "    <https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345>`__.\n",
    "\n",
    "    Args:\n",
    "        ngram_size (:obj:`int`):\n",
    "            All ngrams of size :obj:`ngram_size` can only occur once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_size: int):\n",
    "        if not isinstance(ngram_size, int) or ngram_size <= 0:\n",
    "            raise ValueError(f\"`ngram_size` has to be a strictly positive integer, but is {ngram_size}\")\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        num_batch_hypotheses = scores.shape[0]\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_batch_tokens = _calc_banned_ngram_tokens(self.ngram_size, input_ids, num_batch_hypotheses, cur_len)\n",
    "\n",
    "        for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "            scores[i, banned_tokens] = -float(\"inf\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class EncoderNoRepeatNGramLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces no repetition of encoder input ids n-grams for the decoder ids.\n",
    "    See `ParlAI <https://github.com/facebookresearch/ParlAI/blob/master/parlai/core/torch_generator_agent.py#L1350>`__.\n",
    "\n",
    "    Args:\n",
    "        encoder_ngram_size (:obj:`int`):\n",
    "            All ngrams of size :obj:`ngram_size` can only occur within the encoder input ids.\n",
    "        encoder_input_ids (:obj:`int`):\n",
    "            The encoder_input_ids that should not be repeated within the decoder ids.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_ngram_size: int, encoder_input_ids: torch.LongTensor):\n",
    "        if not isinstance(encoder_ngram_size, int) or encoder_ngram_size <= 0:\n",
    "            raise ValueError(\n",
    "                f\"`encoder_ngram_size` has to be a strictly positive integer, but is {encoder_ngram_size}\"\n",
    "            )\n",
    "        self.ngram_size = encoder_ngram_size\n",
    "        if len(encoder_input_ids.shape) == 1:\n",
    "            encoder_input_ids = encoder_input_ids.unsqueeze(0)\n",
    "        self.batch_size = encoder_input_ids.shape[0]\n",
    "        self.generated_ngrams = _get_ngrams(encoder_ngram_size, encoder_input_ids, self.batch_size)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # B x num_beams\n",
    "        num_hypos = scores.shape[0]\n",
    "        num_beams = num_hypos // self.batch_size\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_batch_tokens = [\n",
    "            _get_generated_ngrams(\n",
    "                self.generated_ngrams[hypo_idx // num_beams], input_ids[hypo_idx], self.ngram_size, cur_len\n",
    "            )\n",
    "            for hypo_idx in range(num_hypos)\n",
    "        ]\n",
    "\n",
    "        for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "            scores[i, banned_tokens] = -float(\"inf\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class NoBadWordsLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces that specified sequences will never be sampled.\n",
    "\n",
    "    Args:\n",
    "        bad_words_ids (:obj:`List[List[int]]`):\n",
    "            List of list of token ids that are not allowed to be generated. In order to get the tokens of the words\n",
    "            that should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
    "            add_prefix_space=True).input_ids`.\n",
    "        eos_token_id (:obj:`int`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bad_words_ids: Iterable[Iterable[int]], eos_token_id: int):\n",
    "\n",
    "        if not isinstance(bad_words_ids, List) or len(bad_words_ids) == 0:\n",
    "            raise ValueError(f\"`bad_words_ids` has to be a non-emtpy list, but is {bad_words_ids}.\")\n",
    "        if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):\n",
    "            raise ValueError(f\"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}.\")\n",
    "        if any(\n",
    "            any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in bad_word_ids)\n",
    "            for bad_word_ids in bad_words_ids\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Each list in `bad_words_ids` has to be a list of positive integers, but is {bad_words_ids}.\"\n",
    "            )\n",
    "\n",
    "        self.bad_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))\n",
    "\n",
    "        for banned_token_seq in self.bad_words_ids:\n",
    "            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n",
    "                bad_words_ids\n",
    "            )\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        banned_tokens = self._calc_banned_bad_words_ids(input_ids)\n",
    "        scores = self._set_scores_to_inf_for_banned_tokens(scores, banned_tokens)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _tokens_match(self, prev_tokens: torch.LongTensor, tokens: List[int]) -> bool:\n",
    "        if len(tokens) == 0:\n",
    "            # if bad word tokens is just one token always ban it\n",
    "            return True\n",
    "        elif len(tokens) > len(prev_tokens):\n",
    "            # if bad word tokens are longer then prev input_ids they can't be equal\n",
    "            return False\n",
    "        elif prev_tokens[-len(tokens) :].tolist() == tokens:\n",
    "            # if tokens match\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _calc_banned_bad_words_ids(self, prev_input_ids: Iterable[int]) -> Iterable[int]:\n",
    "        banned_tokens = []\n",
    "        for prev_input_ids_slice in prev_input_ids:\n",
    "            banned_tokens_slice = []\n",
    "            for banned_token_seq in self.bad_words_ids:\n",
    "                if self._tokens_match(prev_input_ids_slice, banned_token_seq[:-1]) is False:\n",
    "                    # if tokens do not match continue\n",
    "                    continue\n",
    "\n",
    "                banned_tokens_slice.append(banned_token_seq[-1])\n",
    "\n",
    "            banned_tokens.append(banned_tokens_slice)\n",
    "\n",
    "        return banned_tokens\n",
    "\n",
    "    def _set_scores_to_inf_for_banned_tokens(self, scores: torch.Tensor, banned_tokens: List[List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a\n",
    "        list of list of banned tokens to ban in the format [[batch index, vocabulary position],...\n",
    "\n",
    "        Args:\n",
    "            scores: logits distribution of shape (batch size, vocabulary size)\n",
    "            banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "        \"\"\"\n",
    "        banned_mask_list = []\n",
    "        for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "            for token in batch_banned_tokens:\n",
    "                banned_mask_list.append([idx, token])\n",
    "        if not banned_mask_list:\n",
    "            return scores\n",
    "\n",
    "        banned_mask = torch.LongTensor(banned_mask_list)\n",
    "        indices = torch.ones(len(banned_mask))\n",
    "        # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]]. A conversion to dense tensor generates:\n",
    "        # [ 0  1  1 ]\n",
    "        # [ 0  0  0 ]\n",
    "        # [ 1  0  0 ]\n",
    "\n",
    "        banned_mask = (\n",
    "            torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "        )\n",
    "        scores = scores.masked_fill(banned_mask, -float(\"inf\"))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class PrefixConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces contrained generation and is useful for prefix-conditioned\n",
    "    constrained generation. See `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__ for more\n",
    "    information.\n",
    "\n",
    "    Args:\n",
    "        prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`):\n",
    "            This function constraints the beam search to allowed tokens only at each step. This function takes 2\n",
    "            arguments :obj:`inputs_ids` and the batch ID :obj:`batch_id`. It has to return a list with the allowed\n",
    "            tokens for the next generation step conditioned on the previously generated tokens :obj:`inputs_ids` and\n",
    "            the batch ID :obj:`batch_id`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], num_beams: int):\n",
    "        self._prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n",
    "        self._num_beams = num_beams\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        mask = torch.full_like(scores, -math.inf)\n",
    "        for batch_id, beam_sent in enumerate(input_ids.view(-1, self._num_beams, input_ids.shape[-1])):\n",
    "            for beam_id, sent in enumerate(beam_sent):\n",
    "                mask[batch_id * self._num_beams + beam_id, self._prefix_allowed_tokens_fn(batch_id, sent)] = 0\n",
    "\n",
    "        return scores + mask\n",
    "\n",
    "\n",
    "class HammingDiversityLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.LogitsProcessor` that enforces diverse beam search. Note that this logits processor is only\n",
    "    effective for :meth:`transformers.PretrainedModel.group_beam_search`. See `Diverse Beam Search: Decoding Diverse\n",
    "    Solutions from Neural Sequence Models <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "\n",
    "    Args:\n",
    "        diversity_penalty (:obj:`float`):\n",
    "            This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n",
    "            particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is enabled.\n",
    "        num_beams (:obj:`int`):\n",
    "            Number of beams used for group beam search. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for\n",
    "            more details.\n",
    "        num_beam_groups (:obj:`int`):\n",
    "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "            beams. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diversity_penalty: float, num_beams: int, num_beam_groups: int):\n",
    "        if not isinstance(diversity_penalty, float) or (not diversity_penalty > 0.0):\n",
    "            raise ValueError(\"`diversity_penalty` should be a float strictly larger than 0.\")\n",
    "        self._diversity_penalty = diversity_penalty\n",
    "        if not isinstance(num_beams, int) or num_beams < 2:\n",
    "            raise ValueError(\"`num_beams` should be an integer strictly larger than 1.\")\n",
    "        self._num_beams = num_beams\n",
    "        if not isinstance(num_beam_groups, int) or num_beam_groups < 2:\n",
    "            raise ValueError(\"`num_beam_groups` should be an integer strictly larger than 1.\")\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"`beam_groups` has to be smaller or equal to `num_beams`.\")\n",
    "        self._num_sub_beams = num_beams // num_beam_groups\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        scores: torch.FloatTensor,\n",
    "        current_tokens: torch.LongTensor,\n",
    "        beam_group_idx: int,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # hamming diversity: penalise using same token in current group which was used in previous groups at\n",
    "        # the same time step\n",
    "        batch_size = current_tokens.shape[0] // self._num_beams\n",
    "        group_start_idx = beam_group_idx * self._num_sub_beams\n",
    "        group_end_idx = min(group_start_idx + self._num_sub_beams, self._num_beams)\n",
    "        group_size = group_end_idx - group_start_idx\n",
    "        vocab_size = scores.shape[-1]\n",
    "\n",
    "        if group_start_idx == 0:\n",
    "            return scores\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            # predicted tokens of last time step of previous groups\n",
    "            previous_group_tokens = current_tokens[\n",
    "                batch_idx * self._num_beams : batch_idx * self._num_beams + group_start_idx\n",
    "            ]\n",
    "            token_frequency = torch.bincount(previous_group_tokens, minlength=vocab_size).to(scores.device)\n",
    "            scores[batch_idx * group_size : (batch_idx + 1) * group_size] -= self._diversity_penalty * token_frequency\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.679255Z",
     "start_time": "2021-02-10T06:54:02.652684Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import UserDict\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# from .file_utils import add_start_docstrings\n",
    "\n",
    "\n",
    "PROCESS_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        next_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            Current scores of the top :obj:`2 * num_beams` non-finished beam hypotheses.\n",
    "        next_tokens (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            :obj:`input_ids` of the tokens corresponding to the top :obj:`2 * num_beams` non-finished beam hypotheses.\n",
    "        next_indices (:obj:`torch.LongTensor` of shape :obj:`(batch_size, 2 * num_beams)`):\n",
    "            Beam indices indicating to which beam hypothesis the :obj:`next_tokens` correspond.\n",
    "        pad_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `padding` token.\n",
    "        eos_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "\n",
    "    Return:\n",
    "        :obj:`UserDict`: A dictionary composed of the fields as defined above:\n",
    "\n",
    "            - **next_beam_scores** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Updated\n",
    "              scores of all non-finished beams.\n",
    "            - **next_beam_tokens** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Next tokens\n",
    "              to be added to the non-finished beam_hypotheses.\n",
    "            - **next_beam_indices** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`) -- Beam indices\n",
    "              indicating to which beam the next tokens shall be added.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FINALIZE_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size * num_beams, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using any class inheriting from :class:`~transformers.PretrainedTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        final_beam_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The final scores of all non-finished beams.\n",
    "        final_beam_tokens (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The last tokens to be added to the non-finished beam_hypotheses.\n",
    "        final_beam_indices (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_beams)`):\n",
    "            The beam indices indicating to which beam the :obj:`final_beam_tokens` shall be added.\n",
    "        pad_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `padding` token.\n",
    "        eos_token_id (:obj:`int`, `optional`):\n",
    "            The id of the `end-of-sequence` token.\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.LongTensor` of shape :obj:`(batch_size * num_return_sequences, sequence_length)`: The generated\n",
    "        sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or shorter if all\n",
    "        batches finished early due to the :obj:`eos_token_id`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BeamScorer(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all beam scorers that are used for :meth:`~transformers.PretrainedModel.beam_search` and\n",
    "    :meth:`~transformers.PretrainedModel.beam_sample`.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(PROCESS_INPUTS_DOCSTRING)\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        **kwargs\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(FINALIZE_INPUTS_DOCSTRING)\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        **kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")\n",
    "\n",
    "\n",
    "class BeamSearchScorer(BeamScorer):\n",
    "    r\"\"\"\n",
    "    :class:`transformers.BeamScorer` implementing standard beam search decoding.\n",
    "\n",
    "    Adapted in part from `Facebook's XLM beam search code\n",
    "    <https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529>`__.\n",
    "\n",
    "    Reference for the diverse beam search algorithm and implementation `Ashwin Kalyan's DBS implementation\n",
    "    <https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua>`__\n",
    "\n",
    "    Args:\n",
    "        batch_size (:obj:`int`):\n",
    "            Batch Size of :obj:`input_ids` for which standard beam search decoding is run in parallel.\n",
    "        max_length (:obj:`int`):\n",
    "            The maximum length of the sequence to be generated.\n",
    "        num_beams (:obj:`int`):\n",
    "            Number of beams for beam search.\n",
    "        device (:obj:`torch.device`):\n",
    "            Defines the device type (*e.g.*, :obj:`\"cpu\"` or :obj:`\"cuda\"`) on which this instance of\n",
    "            :obj:`BeamSearchScorer` will be allocated.\n",
    "        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "            sequences.\n",
    "        do_early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "        num_beam_hyps_to_keep (:obj:`int`, `optional`, defaults to 1):\n",
    "            The number of beam hypotheses that shall be returned upon calling\n",
    "            :meth:`~transformer.BeamSearchScorer.finalize`.\n",
    "        num_beam_groups (:obj:`int`):\n",
    "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "            beams. See `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        max_length: int,\n",
    "        num_beams: int,\n",
    "        device: torch.device,\n",
    "        length_penalty: Optional[float] = 1.0,\n",
    "        do_early_stopping: Optional[bool] = False,\n",
    "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
    "        num_beam_groups: Optional[int] = 1,\n",
    "    ):\n",
    "        self.max_length = max_length\n",
    "        self.num_beams = num_beams\n",
    "        self.device = device\n",
    "        self.length_penalty = length_penalty\n",
    "        self.do_early_stopping = do_early_stopping\n",
    "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
    "        self.num_beam_groups = num_beam_groups\n",
    "        self.group_size = self.num_beams // self.num_beam_groups\n",
    "\n",
    "        self._is_init = False\n",
    "        self._beam_hyps = [\n",
    "            BeamHypotheses(\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "                length_penalty=self.length_penalty,\n",
    "                early_stopping=self.do_early_stopping,\n",
    "            )\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "        self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n",
    "\n",
    "        if not isinstance(num_beams, int) or num_beams <= 1:\n",
    "            raise ValueError(\n",
    "                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1, one should make use of `greedy_search` instead.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n",
    "            raise ValueError(\n",
    "                f\"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` \"\n",
    "                f\"has to be divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_done(self) -> bool:\n",
    "        return self._done.all()\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        batch_size = len(self._beam_hyps)\n",
    "        assert batch_size == (input_ids.shape[0] // self.group_size)\n",
    "\n",
    "        device = input_ids.device\n",
    "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
    "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
    "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
    "\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                assert (\n",
    "                    len(beam_hyp) >= self.num_beams\n",
    "                ), \"Batch can only be done if at least {} beams have been generated\".format(self.num_beams)\n",
    "                assert (\n",
    "                    eos_token_id is not None and pad_token_id is not None\n",
    "                ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n",
    "                # pad the batch\n",
    "                next_beam_scores[batch_idx, :] = 0\n",
    "                next_beam_tokens[batch_idx, :] = pad_token_id\n",
    "                next_beam_indices[batch_idx, :] = 0\n",
    "                continue\n",
    "\n",
    "            # next tokens for this sentence\n",
    "            beam_idx = 0\n",
    "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
    "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
    "            ):\n",
    "                batch_beam_idx = batch_idx * self.group_size + next_index\n",
    "                # add to generated hypotheses if end of sentence\n",
    "                if (eos_token_id is not None) and (next_token.item() == eos_token_id):\n",
    "                    # if beam_token does not belong to top num_beams tokens, it should not be added\n",
    "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
    "                    if is_beam_token_worse_than_top_num_beams:\n",
    "                        continue\n",
    "                    beam_hyp.add(\n",
    "                        input_ids[batch_beam_idx].clone(),\n",
    "                        next_score.item(),\n",
    "                    )\n",
    "                else:\n",
    "                    # add next predicted token since it is not eos_token\n",
    "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
    "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
    "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
    "                    beam_idx += 1\n",
    "\n",
    "                # once the beam for next step is full, don't add more tokens to it.\n",
    "                if beam_idx == self.group_size:\n",
    "                    break\n",
    "\n",
    "            if beam_idx < self.group_size:\n",
    "                raise ValueError(\n",
    "                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n",
    "                )\n",
    "\n",
    "            # Check if we are done so that we can save a pad step if all(done)\n",
    "            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n",
    "                next_scores[batch_idx].max().item(), cur_len\n",
    "            )\n",
    "\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
    "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
    "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        final_beam_scores: torch.FloatTensor,\n",
    "        final_beam_tokens: torch.LongTensor,\n",
    "        final_beam_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "    ) -> Tuple[torch.LongTensor]:\n",
    "        batch_size = len(self._beam_hyps)\n",
    "\n",
    "        # finalize all open beam hypotheses and add to generated hypotheses\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                continue\n",
    "\n",
    "            # all open beam hypotheses are added to the beam hypothesis\n",
    "            # beam hypothesis class automatically keeps the best beams\n",
    "            for beam_id in range(self.num_beams):\n",
    "                batch_beam_idx = batch_idx * self.num_beams + beam_id\n",
    "                final_score = final_beam_scores[batch_beam_idx].item()\n",
    "                final_tokens = input_ids[batch_beam_idx]\n",
    "                beam_hyp.add(final_tokens, final_score)\n",
    "\n",
    "        # select the best hypotheses\n",
    "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
    "        best = []\n",
    "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # retrieve best hypotheses\n",
    "        for i, beam_hyp in enumerate(self._beam_hyps):\n",
    "            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n",
    "            for j in range(self.num_beam_hyps_to_keep):\n",
    "                best_hyp_tuple = sorted_hyps.pop()\n",
    "                best_score = best_hyp_tuple[0]\n",
    "                best_hyp = best_hyp_tuple[1]\n",
    "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
    "\n",
    "                # append to lists\n",
    "                best.append(best_hyp)\n",
    "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
    "\n",
    "        # prepare for adding eos\n",
    "        sent_max_len = min(sent_lengths.max().item() + 1, self.max_length)\n",
    "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "        # shorter batches are padded if needed\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            assert pad_token_id is not None, \"`pad_token_id` has to be defined\"\n",
    "            decoded.fill_(pad_token_id)\n",
    "\n",
    "        # fill with hypotheses and eos_token_id if the latter fits in\n",
    "        for i, hypo in enumerate(best):\n",
    "            decoded[i, : sent_lengths[i]] = hypo\n",
    "            if sent_lengths[i] < self.max_length:\n",
    "                decoded[i, sent_lengths[i]] = eos_token_id\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"sequences\": decoded,\n",
    "                \"sequence_scores\": best_scores,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "class BeamHypotheses:\n",
    "    def __init__(self, num_beams: int, max_length: int, length_penalty: float, early_stopping: bool):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring bos_token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.num_beams = num_beams\n",
    "        self.beams = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(self, hyp: torch.LongTensor, sum_logprobs: float):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_next_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_next_scores[0][1]]\n",
    "                self.worst_score = sorted_next_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs: float, cur_len: int) -> bool:\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n",
    "        one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n",
    "            ret = self.worst_score >= cur_score\n",
    "            return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.797596Z",
     "start_time": "2021-02-10T06:54:02.681651Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from .file_utils import ModelOutput\n",
    "# from .generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "# from .generation_logits_process import (\n",
    "#     EncoderNoRepeatNGramLogitsProcessor,\n",
    "#     HammingDiversityLogitsProcessor,\n",
    "#     LogitsProcessorList,\n",
    "#     MinLengthLogitsProcessor,\n",
    "#     NoBadWordsLogitsProcessor,\n",
    "#     NoRepeatNGramLogitsProcessor,\n",
    "#     PrefixConstrainedLogitsProcessor,\n",
    "#     RepetitionPenaltyLogitsProcessor,\n",
    "#     TemperatureLogitsWarper,\n",
    "#     TopKLogitsWarper,\n",
    "#     TopPLogitsWarper,\n",
    "# )\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using greedy search. Hidden states and attention\n",
    "    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the\n",
    "    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using sampling.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using sampling. Hidden states and attention weights of\n",
    "    the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states\n",
    "    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of\n",
    "            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape\n",
    "            :obj:`(batch_size*num_return_sequences, num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_return_sequences, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using beam search.\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,\n",
    "            hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using beam search. Hidden states and attention weights\n",
    "    of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states\n",
    "    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, num_heads,\n",
    "            generated_length, sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,\n",
    "            hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSampleDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using beam sample.\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).\n",
    "        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamSampleEncoderDecoderOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of encoder-decoder generation models using beam sampling. Hidden states and attention\n",
    "    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the\n",
    "    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)\n",
    "\n",
    "    Args:\n",
    "        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_beams, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or\n",
    "            shorter if all batches finished early due to the :obj:`eos_token_id`.\n",
    "        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Final beam scores of the generated ``sequences``.\n",
    "        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):\n",
    "            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log\n",
    "            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam\n",
    "            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape\n",
    "            :obj:`(batch_size*num_beams, config.vocab_size)`).\n",
    "        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,\n",
    "            num_heads, sequence_length, sequence_length)`.\n",
    "        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size*num_beams, sequence_length, hidden_size)`.\n",
    "        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,\n",
    "            sequence_length)`.\n",
    "        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    sequences_scores: Optional[torch.FloatTensor] = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n",
    "GreedySearchOutput = Union[GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput]\n",
    "SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
    "BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
    "BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]\n",
    "\n",
    "\n",
    "class GenerationMixin:\n",
    "    \"\"\"\n",
    "    A class containing all of the functions supporting generation, to be used as a mixin in\n",
    "    :class:`~transformers.PreTrainedModel`.\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the\n",
    "        generate method.\n",
    "        \"\"\"\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "    def adjust_logits_during_generation(self, logits: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in\n",
    "        the generate method.\n",
    "        \"\"\"\n",
    "        return logits\n",
    "\n",
    "    def _prepare_input_ids_for_generation(self, bos_token_id: int) -> torch.LongTensor:\n",
    "        if bos_token_id is None:\n",
    "            raise ValueError(\"`bos_token_id` has to be defined when no `input_ids` are provided.\")\n",
    "        return torch.ones((1, 1), dtype=torch.long, device=self.device) * bos_token_id\n",
    "\n",
    "    def _prepare_attention_mask_for_generation(\n",
    "        self, input_ids: torch.Tensor, pad_token_id: int, eos_token_id: int\n",
    "    ) -> torch.LongTensor:\n",
    "        is_pad_token_in_inputs_ids = (pad_token_id is not None) and (pad_token_id in input_ids)\n",
    "        is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or (\n",
    "            (eos_token_id is not None) and (pad_token_id != eos_token_id)\n",
    "        )\n",
    "        if is_pad_token_in_inputs_ids and is_pad_token_not_equal_to_eos_token_id:\n",
    "            return input_ids.ne(pad_token_id).long()\n",
    "        return input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "    def _prepare_encoder_decoder_kwargs_for_generation(\n",
    "        self, input_ids: torch.LongTensor, model_kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        # retrieve encoder hidden states\n",
    "        encoder = self.get_encoder()\n",
    "        encoder_kwargs = {\n",
    "            argument: value for argument, value in model_kwargs.items() if not argument.startswith(\"decoder_\")\n",
    "        }\n",
    "        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(input_ids, return_dict=True, **encoder_kwargs)\n",
    "        return model_kwargs\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(\n",
    "        self, input_ids: torch.LongTensor, decoder_start_token_id: int = None, bos_token_id: int = None\n",
    "    ) -> torch.LongTensor:\n",
    "        decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)\n",
    "        decoder_input_ids = (\n",
    "            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)\n",
    "            * decoder_start_token_id\n",
    "        )\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def _get_pad_token_id(self, pad_token_id: int = None, eos_token_id: int = None) -> int:\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "        return pad_token_id\n",
    "\n",
    "    def _get_decoder_start_token_id(self, decoder_start_token_id: int = None, bos_token_id: int = None) -> int:\n",
    "        decoder_start_token_id = (\n",
    "            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n",
    "        )\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "\n",
    "        if decoder_start_token_id is not None:\n",
    "            return decoder_start_token_id\n",
    "        elif (\n",
    "            hasattr(self.config, \"decoder\")\n",
    "            and hasattr(self.config.decoder, \"decoder_start_token_id\")\n",
    "            and self.config.decoder.decoder_start_token_id is not None\n",
    "        ):\n",
    "            return self.config.decoder.decoder_start_token_id\n",
    "        elif bos_token_id is not None:\n",
    "            return bos_token_id\n",
    "        elif (\n",
    "            hasattr(self.config, \"decoder\")\n",
    "            and hasattr(self.config.decoder, \"bos_token_id\")\n",
    "            and self.config.decoder.bos_token_id is not None\n",
    "        ):\n",
    "            return self.config.decoder.bos_token_id\n",
    "        raise ValueError(\n",
    "            \"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _expand_inputs_for_generation(\n",
    "        input_ids: torch.LongTensor,\n",
    "        expand_size: int = 1,\n",
    "        is_encoder_decoder: bool = False,\n",
    "        attention_mask: torch.LongTensor = None,\n",
    "        encoder_outputs: ModelOutput = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n",
    "        expanded_return_idx = (\n",
    "            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\n",
    "        )\n",
    "        input_ids = input_ids.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = token_type_ids.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            model_kwargs[\"attention_mask\"] = attention_mask.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if is_encoder_decoder:\n",
    "            assert encoder_outputs is not None\n",
    "            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "                0, expanded_return_idx.to(encoder_outputs.last_hidden_state.device)\n",
    "            )\n",
    "            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n",
    "        return input_ids, model_kwargs\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_sequence_length_for_generation(\n",
    "        input_ids: torch.LongTensor, max_length: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "        sequence_lengths = input_ids.new(input_ids.shape[0]).fill_(max_length)\n",
    "\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        return sequence_lengths, unfinished_sequences, cur_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_seq_length_for_generation(\n",
    "        sequence_lengths: torch.LongTensor,\n",
    "        unfinished_sequences: torch.LongTensor,\n",
    "        cur_len: int,\n",
    "        is_eos_in_next_token: torch.BoolTensor,\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        # check if sentence is not finished yet\n",
    "        is_sent_unfinished = unfinished_sequences.mul(is_eos_in_next_token.long()).bool()\n",
    "\n",
    "        # update sentence length\n",
    "        sequence_lengths = sequence_lengths.masked_fill(is_sent_unfinished, cur_len)\n",
    "        unfinished_sequences = unfinished_sequences.mul((~is_eos_in_next_token).long())\n",
    "        return sequence_lengths, unfinished_sequences\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_model_kwargs_for_generation(\n",
    "        outputs: ModelOutput, model_kwargs: Dict[str, Any], is_encoder_decoder: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        # update past\n",
    "        if \"past_key_values\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.past_key_values\n",
    "        elif \"mems\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.mems\n",
    "        elif \"past_buckets_states\" in outputs:\n",
    "            model_kwargs[\"past\"] = outputs.past_buckets_states\n",
    "        else:\n",
    "            model_kwargs[\"past\"] = None\n",
    "\n",
    "        # update token_type_ids with last value\n",
    "        if \"token_type_ids\" in model_kwargs:\n",
    "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
    "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        # update attention mask\n",
    "        if not is_encoder_decoder:\n",
    "            if \"attention_mask\" in model_kwargs:\n",
    "                attention_mask = model_kwargs[\"attention_mask\"]\n",
    "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "                )\n",
    "\n",
    "        return model_kwargs\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        raise NotImplementedError(\n",
    "            f\"Make sure that a `_reorder_cache` function is correctly implemented in {self.__class__.__module__} to enable beam search for {self.__class__}\"\n",
    "        )\n",
    "\n",
    "    def _get_logits_warper(\n",
    "        self, top_k: int = None, top_p: float = None, temperature: float = None, num_beams: int = None\n",
    "    ) -> LogitsProcessorList:\n",
    "        \"\"\"\n",
    "        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\n",
    "        :obj:`~transformers.LogitsWarper` instances used for multinomial sampling.\n",
    "        \"\"\"\n",
    "\n",
    "        # init warp parameters\n",
    "        top_k = top_k if top_k is not None else self.config.top_k\n",
    "        top_p = top_p if top_p is not None else self.config.top_p\n",
    "        temperature = temperature if temperature is not None else self.config.temperature\n",
    "        # instantiate warpers list\n",
    "        warpers = LogitsProcessorList()\n",
    "\n",
    "        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\n",
    "        # all samplers can be found in `generation_utils_samplers.py`\n",
    "        if temperature is not None and temperature != 1.0:\n",
    "            warpers.append(TemperatureLogitsWarper(temperature))\n",
    "        if top_k is not None and top_k != 0:\n",
    "            warpers.append(TopKLogitsWarper(top_k=top_k, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "        if top_p is not None and top_p < 1.0:\n",
    "            warpers.append(TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "        return warpers\n",
    "\n",
    "    def _get_logits_processor(\n",
    "        self,\n",
    "        repetition_penalty: float,\n",
    "        no_repeat_ngram_size: int,\n",
    "        encoder_no_repeat_ngram_size: int,\n",
    "        encoder_input_ids: torch.LongTensor,\n",
    "        bad_words_ids: List[List[int]],\n",
    "        min_length: int,\n",
    "        eos_token_id: int,\n",
    "        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]],\n",
    "        num_beams: int,\n",
    "        num_beam_groups: int,\n",
    "        diversity_penalty: float,\n",
    "    ) -> LogitsProcessorList:\n",
    "        \"\"\"\n",
    "        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant\n",
    "        :obj:`~transformers.LogitsProcessor` instances used to modify the scores of the language model head.\n",
    "        \"\"\"\n",
    "\n",
    "        # init warp parameters\n",
    "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
    "        no_repeat_ngram_size = (\n",
    "            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
    "        )\n",
    "        encoder_no_repeat_ngram_size = (\n",
    "            encoder_no_repeat_ngram_size\n",
    "            if encoder_no_repeat_ngram_size is not None\n",
    "            else self.config.encoder_no_repeat_ngram_size\n",
    "        )\n",
    "        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n",
    "        min_length = min_length if min_length is not None else self.config.min_length\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        diversity_penalty = diversity_penalty if diversity_penalty is not None else self.config.diversity_penalty\n",
    "        # instantiate processors list\n",
    "        processors = LogitsProcessorList()\n",
    "\n",
    "        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files\n",
    "        # all samplers can be found in `generation_utils_samplers.py`\n",
    "        if diversity_penalty is not None and diversity_penalty > 0.0:\n",
    "            processors.append(\n",
    "                HammingDiversityLogitsProcessor(\n",
    "                    diversity_penalty=diversity_penalty, num_beams=num_beams, num_beam_groups=num_beam_groups\n",
    "                )\n",
    "            )\n",
    "        if repetition_penalty is not None and repetition_penalty != 1.0:\n",
    "            processors.append(RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty))\n",
    "        if no_repeat_ngram_size is not None and no_repeat_ngram_size > 0:\n",
    "            processors.append(NoRepeatNGramLogitsProcessor(no_repeat_ngram_size))\n",
    "        if encoder_no_repeat_ngram_size is not None and encoder_no_repeat_ngram_size > 0:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                processors.append(EncoderNoRepeatNGramLogitsProcessor(encoder_no_repeat_ngram_size, encoder_input_ids))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\"\n",
    "                )\n",
    "        if bad_words_ids is not None:\n",
    "            processors.append(NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id))\n",
    "        if min_length is not None and eos_token_id is not None and min_length > -1:\n",
    "            processors.append(MinLengthLogitsProcessor(min_length, eos_token_id))\n",
    "        if prefix_allowed_tokens_fn is not None:\n",
    "            processors.append(PrefixConstrainedLogitsProcessor(prefix_allowed_tokens_fn, num_beams))\n",
    "        return processors\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        num_beam_groups: Optional[int] = None,\n",
    "        diversity_penalty: Optional[float] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "        multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
    "\n",
    "        Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
    "        attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
    "        indicated are the default values of those config.\n",
    "\n",
    "        Most of these parameters are explained in more detail in `this blog post\n",
    "        <https://huggingface.co/blog/how-to-generate>`__.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            min_length (:obj:`int`, `optional`, defaults to 10):\n",
    "                The minimum length of the sequence to be generated.\n",
    "            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "            early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "            num_beams (:obj:`int`, `optional`, defaults to 1):\n",
    "                Number of beams for beam search. 1 means no beam search.\n",
    "            temperature (:obj:`float`, `optional`, defaults tp 1.0):\n",
    "                The value used to module the next token probabilities.\n",
    "            top_k (:obj:`int`, `optional`, defaults to 50):\n",
    "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
    "                higher are kept for generation.\n",
    "            repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "                <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            bos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `beginning-of-sequence` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "                model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "                sequences.\n",
    "            no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size can only occur once.\n",
    "            encoder_no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the\n",
    "                ``decoder_input_ids``.\n",
    "            bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
    "                List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
    "                should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
    "                add_prefix_space=True).input_ids`.\n",
    "            num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
    "                The number of independently computed returned sequences for each element in the batch.\n",
    "            attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
    "                tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
    "                shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
    "                <../glossary.html#attention-mask>`__\n",
    "            decoder_start_token_id (:obj:`int`, `optional`):\n",
    "                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
    "            use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "                speed up decoding.\n",
    "            num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
    "                Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
    "                beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
    "            diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
    "                This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
    "                at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
    "                enabled.\n",
    "            prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
    "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
    "                provided no constraint is applied. This function takes 2 arguments :obj:`inputs_ids` and the batch ID\n",
    "                :obj:`batch_id`. It has to return a list with the allowed tokens for the next generation step\n",
    "                conditioned on the previously generated tokens :obj:`inputs_ids` and the batch ID :obj:`batch_id`. This\n",
    "                argument is useful for constrained generation conditioned on the prefix, as described in\n",
    "                `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
    "                model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
    "                kwargs should be prefixed with `decoder_`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
    "            :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
    "            ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
    "\n",
    "                If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
    "                possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
    "\n",
    "                If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
    "                :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
    "\n",
    "        Examples::\n",
    "            >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "            >>> # do greedy decoding without providing a prompt\n",
    "            >>> outputs = model.generate(max_length=40)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "            >>> document = (\n",
    "            ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
    "            ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
    "            ... )\n",
    "            >>> # encode input contex\n",
    "            >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
    "            >>> # with T5 encoder-decoder model conditioned on short news article.\n",
    "            >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "            >>> input_context = \"The dog\"\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate 3 candidates using sampling\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
    "            >>> # \"Legal\" is one of the control codes for ctrl\n",
    "            >>> input_context = \"Legal My neighbor is\"\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "            >>> input_context = \"My cute dog\"\n",
    "            >>> # get tokens of words that should not be generated\n",
    "            >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
    "            >>> # encode input context\n",
    "            >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "            >>> # generate sequences without allowing bad_words to be generated\n",
    "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
    "            >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # set init values\n",
    "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "        num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "        num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"output_attentions\"] = output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "\n",
    "        if input_ids is None:\n",
    "            # init `input_ids` with bos_token_id\n",
    "            input_ids = self._prepare_input_ids_for_generation(bos_token_id)\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "            # init `attention_mask` depending on `pad_token_id`\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "                input_ids, pad_token_id, eos_token_id\n",
    "            )\n",
    "\n",
    "        # special case if pad_token_id is not defined\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "\n",
    "        # Storing encoder_input_ids for logits_processor that could use them\n",
    "        encoder_input_ids = input_ids if self.config.is_encoder_decoder else None\n",
    "\n",
    "        if self.config.is_encoder_decoder:\n",
    "            # add encoder_outputs to model_kwargs\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n",
    "\n",
    "            # set input_ids as decoder_input_ids\n",
    "            if \"decoder_input_ids\" in model_kwargs:\n",
    "                input_ids = model_kwargs.pop(\"decoder_input_ids\")\n",
    "            else:\n",
    "                input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "                    input_ids, decoder_start_token_id=decoder_start_token_id, bos_token_id=bos_token_id\n",
    "                )\n",
    "\n",
    "            if \"encoder_outputs\" not in model_kwargs or not isinstance(model_kwargs[\"encoder_outputs\"], ModelOutput):\n",
    "                raise ValueError(\"Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.\")\n",
    "\n",
    "        if input_ids.shape[-1] >= max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "            logger.warning(\n",
    "                f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}.\"\n",
    "                \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "            )\n",
    "\n",
    "        # determine generation mode\n",
    "        is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "        if is_group_beam_gen_mode and do_sample is True:\n",
    "            raise ValueError(\n",
    "                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "            )\n",
    "\n",
    "        # set model_kwargs\n",
    "        model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "        # get distribution pre_processing samplers\n",
    "        logits_processor = self._get_logits_processor(\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "            encoder_input_ids=encoder_input_ids,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            min_length=min_length,\n",
    "            eos_token_id=eos_token_id,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "        )\n",
    "\n",
    "        if is_greedy_gen_mode:\n",
    "            if num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "                )\n",
    "\n",
    "            # greedy search\n",
    "            return self.greedy_search(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_sample_gen_mode:\n",
    "            # get probability distribution warper\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            # expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # sample\n",
    "            return self.sample(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_gen_mode:\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "            )\n",
    "            # interleave with `num_beams`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            return self.beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_sample_gen_mode:\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            batch_size = input_ids.shape[0] * num_return_sequences\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "            )\n",
    "\n",
    "            # interleave with `num_beams * num_return_sequences`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_beams * num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            return self.beam_sample(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_group_beam_gen_mode:\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "            early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if num_beams % num_beam_groups != 0:\n",
    "                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "            diverse_beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "                num_beam_groups=num_beam_groups,\n",
    "            )\n",
    "            # interleave with `num_beams`\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            return self.group_beam_search(\n",
    "                input_ids,\n",
    "                diverse_beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "    def greedy_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GreedySearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using greedy decoding.\n",
    "\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "            model_kwargs:\n",
    "                Additional model specific keyword arguments will be forwarded to the :obj:`forward` function of the\n",
    "                model. If model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ... AutoTokenizer,\n",
    "            ... AutoModelForCausalLM,\n",
    "            ... LogitsProcessorList,\n",
    "            ... MinLengthLogitsProcessor,\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "            >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "            >>> model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "            >>> input_prompt = \"Today is a beautiful day, and\"\n",
    "            >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # init sequence length tensors\n",
    "        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\n",
    "            input_ids, max_length\n",
    "        )\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            # prepare model inputs\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_logits,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "            # argmax\n",
    "            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "            # add code that transfomers next_tokens to tokens_to_add\n",
    "            if eos_token_id is not None:\n",
    "                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\n",
    "                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "            # update sequence length\n",
    "            if eos_token_id is not None:\n",
    "                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\n",
    "                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "                )\n",
    "\n",
    "            # update model kwargs\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sequences.max() == 0:\n",
    "                break\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return GreedySearchEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return GreedySearchDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        logits_warper: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[SampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using multinomial sampling.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
    "                modeling head applied before multinomial sampling at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForCausalLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    TopKLogitsWarper,\n",
    "            ...    TemperatureLogitsWarper,\n",
    "            ... )\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "            >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "            >>> model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "            >>> input_prompt = \"Today is a beautiful day, and\"\n",
    "            >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_warper = LogitsProcessorList([\n",
    "            ...     TopKLogitsWarper(50),\n",
    "            ...     TemperatureLogitsWarper(0.7),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # init sequence length tensors\n",
    "        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(\n",
    "            input_ids, max_length\n",
    "        )\n",
    "\n",
    "        # auto-regressive generation\n",
    "        while cur_len < max_length:\n",
    "            # prepare model inputs\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # sample\n",
    "            probs = F.softmax(next_token_scores, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "            # add code that transfomers next_tokens to tokens_to_add\n",
    "            if eos_token_id is not None:\n",
    "                assert pad_token_id is not None, \"If eos_token_id is defined, make sure that pad_token_id is defined.\"\n",
    "                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)\n",
    "\n",
    "            # add token and increase length by one\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # update sequence length\n",
    "            if eos_token_id is not None:\n",
    "                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(\n",
    "                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id\n",
    "                )\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sequences.max() == 0:\n",
    "                break\n",
    "\n",
    "            # update model kwargs\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return SampleEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return SampleDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[BeamSearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search decoding.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForSeq2SeqLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "            >>> # lets run beam search using 3 beams\n",
    "            >>> num_beams = 3\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        assert (\n",
    "            num_beams * batch_size == batch_beam_size\n",
    "        ), \"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # adjust tokens for Bart, *e.g.*\n",
    "            next_token_logits = self.adjust_logits_during_generation(\n",
    "                next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "            )\n",
    "\n",
    "            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores = logits_processor(input_ids, next_token_scores)\n",
    "            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "            next_indices = next_tokens // vocab_size\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    def beam_sample(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        logits_warper: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[BeamSampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search with multinomial sampling.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                A derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
    "                modeling head applied before multinomial sampling at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...     AutoTokenizer,\n",
    "            ...     AutoModelForSeq2SeqLM,\n",
    "            ...     LogitsProcessorList,\n",
    "            ...     MinLengthLogitsProcessor,\n",
    "            ...     TopKLogitsWarper,\n",
    "            ...     TemperatureLogitsWarper,\n",
    "            ...     BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            >>> # lets run beam search using 3 beams\n",
    "            >>> num_beams = 3\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)\n",
    "            ... ])\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_warper = LogitsProcessorList([\n",
    "            ...     TopKLogitsWarper(50),\n",
    "            ...     TemperatureLogitsWarper(0.7),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.beam_sample(\n",
    "            ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
    "            ... )\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # adjust token scores (a no-op by default)\n",
    "            next_token_logits = self.adjust_logits_during_generation(\n",
    "                next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "            )\n",
    "\n",
    "            next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores = logits_processor(input_ids, next_token_scores)\n",
    "            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            probs = F.softmax(next_token_scores, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)\n",
    "            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
    "\n",
    "            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
    "            next_tokens = torch.gather(next_tokens, -1, _indices)\n",
    "\n",
    "            next_indices = next_tokens // vocab_size\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "            )\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    def group_beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head using beam search decoding.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n",
    "                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n",
    "            beam_scorer (:obj:`BeamScorer`):\n",
    "                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
    "                constructed, stored and sorted during generation. For more information, the documentation of\n",
    "                :class:`~transformers.BeamScorer` should be read.\n",
    "            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
    "                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
    "                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
    "                head applied at each generation step.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs that will be forwarded to the :obj:`forward` function of the model. If\n",
    "                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
    "            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
    "            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
    "            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
    "            ``model.config.is_encoder_decoder=True``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import (\n",
    "            ...    AutoTokenizer,\n",
    "            ...    AutoModelForSeq2SeqLM,\n",
    "            ...    LogitsProcessorList,\n",
    "            ...    MinLengthLogitsProcessor,\n",
    "            ...    HammingDiversityLogitsProcessor,\n",
    "            ...    BeamSearchScorer,\n",
    "            ... )\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "            >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "            >>> # lets run diverse beam search using 6 beams\n",
    "            >>> num_beams = 6\n",
    "            >>> # define decoder start token ids\n",
    "            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "            >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "            >>> # add encoder_outputs to model keyword arguments\n",
    "            >>> model_kwargs = {\n",
    "            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
    "            ... }\n",
    "\n",
    "            >>> # instantiate beam scorer\n",
    "            >>> beam_scorer = BeamSearchScorer(\n",
    "            ...     batch_size=1,\n",
    "            ...     max_length=model.config.max_length,\n",
    "            ...     num_beams=num_beams,\n",
    "            ...     device=model.device,\n",
    "            ...     num_beam_groups=3\n",
    "            ... )\n",
    "\n",
    "            >>> # instantiate logits processors\n",
    "            >>> logits_processor = LogitsProcessorList([\n",
    "            ...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
    "            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "            ... ])\n",
    "\n",
    "            >>> outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        \"\"\"\n",
    "\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "        num_beam_groups = beam_scorer.num_beam_groups\n",
    "        num_sub_beams = num_beams // num_beam_groups\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        assert (\n",
    "            num_beams * batch_size == batch_beam_size\n",
    "        ), f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "\n",
    "        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in\n",
    "        # the same group don't produce same tokens everytime.\n",
    "        beam_scores[:, ::num_sub_beams] = 0\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while cur_len < max_length:\n",
    "            # predicted tokens in cur_len step\n",
    "            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "            # indices which will form the beams in the next time step\n",
    "            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "            # do one decoder step on all beams of all sentences in batch\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "            for beam_group_idx in range(num_beam_groups):\n",
    "                group_start_idx = beam_group_idx * num_sub_beams\n",
    "                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "                group_size = group_end_idx - group_start_idx\n",
    "\n",
    "                # indices of beams of current group among all sentences in batch\n",
    "                batch_group_indices = []\n",
    "\n",
    "                if output_scores:\n",
    "                    processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n",
    "\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_group_indices.extend(\n",
    "                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                    )\n",
    "                group_input_ids = input_ids[batch_group_indices]\n",
    "\n",
    "                # select outputs of beams of current group only\n",
    "                next_token_logits = outputs.logits[batch_group_indices, -1, :]\n",
    "\n",
    "                # adjust tokens for Bart, *e.g.*\n",
    "                next_token_logits = self.adjust_logits_during_generation(\n",
    "                    next_token_logits, cur_len=cur_len, max_length=max_length\n",
    "                )\n",
    "\n",
    "                next_token_scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * group_size, vocab_size)\n",
    "                vocab_size = next_token_scores.shape[-1]\n",
    "\n",
    "                next_token_scores = logits_processor(\n",
    "                    group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "                )\n",
    "                next_token_scores = next_token_scores + beam_scores[batch_group_indices].unsqueeze(-1).expand_as(\n",
    "                    next_token_scores\n",
    "                )\n",
    "\n",
    "                if output_scores:\n",
    "                    processed_score[batch_group_indices] = next_token_scores\n",
    "\n",
    "                # reshape for beam search\n",
    "                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "                next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "                next_indices = next_tokens // vocab_size\n",
    "                next_tokens = next_tokens % vocab_size\n",
    "\n",
    "                # stateless\n",
    "                beam_outputs = beam_scorer.process(\n",
    "                    group_input_ids,\n",
    "                    next_token_scores,\n",
    "                    next_tokens,\n",
    "                    next_indices,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id,\n",
    "                )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * (beam_idx // group_size) + group_start_idx + (beam_idx % group_size)\n",
    "                )\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (processed_score,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], reordering_indices)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "            cur_len = cur_len + 1\n",
    "            if beam_scorer.is_done:\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"]\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: torch.FloatTensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        logits = TopKLogitsWarper(top_k=top_k, filter_value=filter_value, min_tokens_to_keep=min_tokens_to_keep)(\n",
    "            None, logits\n",
    "        )\n",
    "\n",
    "    if 0 <= top_p <= 1.0:\n",
    "        logits = TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=min_tokens_to_keep)(None, logits)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:02.931099Z",
     "start_time": "2021-02-10T06:54:02.799571Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, device, dtype, nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from .activations import get_activation\n",
    "# from .configuration_utils import PretrainedConfig\n",
    "# from .file_utils import (\n",
    "#     DUMMY_INPUTS,\n",
    "#     TF2_WEIGHTS_NAME,\n",
    "#     TF_WEIGHTS_NAME,\n",
    "#     WEIGHTS_NAME,\n",
    "#     ModelOutput,\n",
    "#     cached_path,\n",
    "#     hf_bucket_url,\n",
    "#     is_remote_url,\n",
    "#     is_torch_tpu_available,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "# from .generation_utils import GenerationMixin\n",
    "# from .utils import logging\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "try:\n",
    "    from torch.nn import Identity\n",
    "except ImportError:\n",
    "    # Older PyTorch compatibility\n",
    "    class Identity(nn.Module):\n",
    "        r\"\"\"A placeholder identity operator that is argument-insensitive.\"\"\"\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input):\n",
    "            return input\n",
    "\n",
    "\n",
    "def find_pruneable_heads_and_indices(\n",
    "    heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]\n",
    ") -> Tuple[Set[int], torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.\n",
    "\n",
    "    Args:\n",
    "        heads (:obj:`List[int]`): List of the indices of heads to prune.\n",
    "        n_heads (:obj:`int`): The number of heads in the model.\n",
    "        head_size (:obj:`int`): The size of each head.\n",
    "        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.\n",
    "    \"\"\"\n",
    "    mask = torch.ones(n_heads, head_size)\n",
    "    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n",
    "    for head in heads:\n",
    "        # Compute how many pruned heads are before the head and move the index accordingly\n",
    "        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
    "        mask[head] = 0\n",
    "    mask = mask.view(-1).contiguous().eq(1)\n",
    "    index: torch.LongTensor = torch.arange(len(mask))[mask].long()\n",
    "    return heads, index\n",
    "\n",
    "\n",
    "class ModuleUtilsMixin:\n",
    "    \"\"\"\n",
    "    A few utilities for :obj:`torch.nn.Modules`, to be used as a mixin.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n",
    "        try:\n",
    "            import psutil\n",
    "        except (ImportError):\n",
    "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem = process.memory_info()\n",
    "        module.mem_rss_pre_forward = mem.rss\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n",
    "        try:\n",
    "            import psutil\n",
    "        except (ImportError):\n",
    "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem = process.memory_info()\n",
    "        module.mem_rss_post_forward = mem.rss\n",
    "        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n",
    "        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n",
    "        return None\n",
    "\n",
    "    def add_memory_hooks(self):\n",
    "        \"\"\"\n",
    "        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
    "\n",
    "        Increase in memory consumption is stored in a :obj:`mem_rss_diff` attribute for each module and can be reset to\n",
    "        zero with :obj:`model.reset_memory_hooks_state()`.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n",
    "            module.register_forward_hook(self._hook_rss_memory_post_forward)\n",
    "        self.reset_memory_hooks_state()\n",
    "\n",
    "    def reset_memory_hooks_state(self):\n",
    "        \"\"\"\n",
    "        Reset the :obj:`mem_rss_diff` attribute of each module (see\n",
    "        :func:`~transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks`).\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            module.mem_rss_diff = 0\n",
    "            module.mem_rss_post_forward = 0\n",
    "            module.mem_rss_pre_forward = 0\n",
    "\n",
    "    @property\n",
    "    def device(self) -> device:\n",
    "        \"\"\"\n",
    "        :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
    "        device).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return next(self.parameters()).device\n",
    "        except StopIteration:\n",
    "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
    "\n",
    "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
    "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
    "                return tuples\n",
    "\n",
    "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
    "            first_tuple = next(gen)\n",
    "            return first_tuple[1].device\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> dtype:\n",
    "        \"\"\"\n",
    "        :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return next(self.parameters()).dtype\n",
    "        except StopIteration:\n",
    "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
    "\n",
    "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
    "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
    "                return tuples\n",
    "\n",
    "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
    "            first_tuple = next(gen)\n",
    "            return first_tuple[1].dtype\n",
    "\n",
    "    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Invert an attention mask (e.g., switches 0. and 1.).\n",
    "\n",
    "        Args:\n",
    "            encoder_attention_mask (:obj:`torch.Tensor`): An attention mask.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The inverted attention mask.\n",
    "        \"\"\"\n",
    "        if encoder_attention_mask.dim() == 3:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "        if encoder_attention_mask.dim() == 2:\n",
    "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n",
    "        # /transformer/transformer_layers.py#L270\n",
    "        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n",
    "        # encoder_extended_attention_mask.transpose(-1, -2))\n",
    "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "\n",
    "        if self.dtype == torch.float16:\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e4\n",
    "        elif self.dtype == torch.float32:\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} not recognized. `dtype` should be set to either `torch.float32` or `torch.float16`\".format(\n",
    "                    self.dtype\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoder_extended_attention_mask\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device) -> Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if self.config.is_decoder:\n",
    "                batch_size, seq_length = input_shape\n",
    "                seq_ids = torch.arange(seq_length, device=device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "                # causal and attention masks must have same type with pytorch version < 1.3\n",
    "                causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "                if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "                    causal_mask = torch.cat(\n",
    "                        [\n",
    "                            torch.ones(\n",
    "                                (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
    "                            ),\n",
    "                            causal_mask,\n",
    "                        ],\n",
    "                        axis=-1,\n",
    "                    )\n",
    "\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def get_head_mask(\n",
    "        self, head_mask: Optional[Tensor], num_hidden_layers: int, is_attention_chunked: bool = False\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Prepare the head mask if needed.\n",
    "\n",
    "        Args:\n",
    "            head_mask (:obj:`torch.Tensor` with shape :obj:`[num_heads]` or :obj:`[num_hidden_layers x num_heads]`, `optional`):\n",
    "                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
    "            num_hidden_layers (:obj:`int`):\n",
    "                The number of hidden layers in the model.\n",
    "            is_attention_chunked: (:obj:`bool`, `optional, defaults to :obj:`False`):\n",
    "                Whether or not the attentions scores are computed by chunks or not.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` with shape :obj:`[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or\n",
    "            list with :obj:`[None]` for each layer.\n",
    "        \"\"\"\n",
    "        if head_mask is not None:\n",
    "            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n",
    "            if is_attention_chunked is True:\n",
    "                head_mask = head_mask.unsqueeze(-1)\n",
    "        else:\n",
    "            head_mask = [None] * num_hidden_layers\n",
    "\n",
    "        return head_mask\n",
    "\n",
    "    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n",
    "        \"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\n",
    "        if head_mask.dim() == 1:\n",
    "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n",
    "        elif head_mask.dim() == 2:\n",
    "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
    "        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n",
    "        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n",
    "        return head_mask\n",
    "\n",
    "    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
    "\n",
    "        Args:\n",
    "            only_trainable (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return only the number of trainable parameters\n",
    "\n",
    "            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return only the number of non-embeddings parameters\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The number of parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        def parameter_filter(x):\n",
    "            return (x.requires_grad or not only_trainable) and not (\n",
    "                isinstance(x, torch.nn.Embedding) and exclude_embeddings\n",
    "            )\n",
    "\n",
    "        params = filter(parameter_filter, self.parameters()) if only_trainable else self.parameters()\n",
    "        return sum(p.numel() for p in params)\n",
    "\n",
    "    def estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int:\n",
    "        \"\"\"\n",
    "        Helper function to estimate the total number of tokens from the model inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs (:obj:`dict`): The model inputs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The total number of tokens.\n",
    "        \"\"\"\n",
    "        token_inputs = [tensor for key, tensor in input_dict.items() if \"input\" in key]\n",
    "        if token_inputs:\n",
    "            return sum([token_input.numel() for token_input in token_inputs])\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n",
    "            )\n",
    "            return 0\n",
    "\n",
    "    def floating_point_ops(\n",
    "        self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
    "        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
    "        tokens (valid if :obj:`12 * d_model << sequence_length`) as laid out in `this paper\n",
    "        <https://arxiv.org/pdf/2001.08361.pdf>`__ section 2.1. Should be overridden for transformers with parameter\n",
    "        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
    "\n",
    "        Args:\n",
    "            batch_size (:obj:`int`):\n",
    "                The batch size for the forward pass.\n",
    "\n",
    "            sequence_length (:obj:`int`):\n",
    "                The number of tokens in each line of the batch.\n",
    "\n",
    "            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not to count embedding and softmax operations.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`int`: The number of floating-point operations.\n",
    "        \"\"\"\n",
    "\n",
    "        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n",
    "\n",
    "\n",
    "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n",
    "    r\"\"\"\n",
    "    Base class for all models.\n",
    "\n",
    "    :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods\n",
    "    for loading, downloading and saving models as well as a few methods common to all models to:\n",
    "\n",
    "        * resize the input embeddings,\n",
    "        * prune heads in the self-attention heads.\n",
    "\n",
    "    Class attributes (overridden by derived classes):\n",
    "\n",
    "        - **config_class** (:class:`~transformers.PretrainedConfig`) -- A subclass of\n",
    "          :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.\n",
    "        - **load_tf_weights** (:obj:`Callable`) -- A python `method` for loading a TensorFlow checkpoint in a PyTorch\n",
    "          model, taking as arguments:\n",
    "\n",
    "            - **model** (:class:`~transformers.PreTrainedModel`) -- An instance of the model on which to load the\n",
    "              TensorFlow checkpoint.\n",
    "            - **config** (:class:`~transformers.PreTrainedConfig`) -- An instance of the configuration associated to\n",
    "              the model.\n",
    "            - **path** (:obj:`str`) -- A path to the TensorFlow checkpoint.\n",
    "\n",
    "        - **base_model_prefix** (:obj:`str`) -- A string indicating the attribute associated to the base model in\n",
    "          derived classes of the same architecture adding modules on top of the base model.\n",
    "        - **is_parallelizable** (:obj:`bool`) -- A flag indicating whether this model supports model parallelization.\n",
    "    \"\"\"\n",
    "    config_class = None\n",
    "    base_model_prefix = \"\"\n",
    "    # a list of re pattern of tensor names to ignore from the model when loading the model weights\n",
    "    # (and avoid unnecessary warnings).\n",
    "    _keys_to_ignore_on_load_missing = None\n",
    "    # a list of re pattern of tensor names to ignore from the weights when loading the model weights\n",
    "    # (and avoid unnecessary warnings).\n",
    "    _keys_to_ignore_on_load_unexpected = None\n",
    "    # a list of of tensor names to ignore when saving the model (useful for keys that aren't\n",
    "    # trained, but which are deterministic)\n",
    "    _keys_to_ignore_on_save = None\n",
    "\n",
    "    is_parallelizable = False\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
    "        \"\"\"\n",
    "        return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n",
    "                \"To create a model from a pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "        # Save config and origin of the pretrained weights if given in model\n",
    "        self.config = config\n",
    "        self.name_or_path = config.name_or_path\n",
    "\n",
    "    @property\n",
    "    def base_model(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        :obj:`torch.nn.Module`: The main body of the model.\n",
    "        \"\"\"\n",
    "        return getattr(self, self.base_model_prefix, self)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Returns the model's input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`nn.Module`: A torch module mapping vocabulary to hidden states.\n",
    "        \"\"\"\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            return base_model.get_input_embeddings()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        \"\"\"\n",
    "        Set model's input embeddings.\n",
    "\n",
    "        Args:\n",
    "            value (:obj:`nn.Module`): A module mapping vocabulary to hidden states.\n",
    "        \"\"\"\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            base_model.set_input_embeddings(value)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Returns the model's output embeddings.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`nn.Module`: A torch module mapping hidden states to vocabulary.\n",
    "        \"\"\"\n",
    "        return None  # Overwrite for models with output embeddings\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\"\n",
    "        Tie the weights between the input embeddings and the output embeddings.\n",
    "\n",
    "        If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
    "        the weights instead.\n",
    "        \"\"\"\n",
    "        output_embeddings = self.get_output_embeddings()\n",
    "        if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "\n",
    "        if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "            if hasattr(self, self.base_model_prefix):\n",
    "                self = getattr(self, self.base_model_prefix)\n",
    "            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "\n",
    "    @staticmethod\n",
    "    def _tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str):\n",
    "        uninitialized_encoder_weights: List[str] = []\n",
    "        if decoder.__class__ != encoder.__class__:\n",
    "            logger.info(\n",
    "                f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\n",
    "            )\n",
    "\n",
    "        def tie_encoder_to_decoder_recursively(\n",
    "            decoder_pointer: nn.Module,\n",
    "            encoder_pointer: nn.Module,\n",
    "            module_name: str,\n",
    "            uninitialized_encoder_weights: List[str],\n",
    "            depth=0,\n",
    "        ):\n",
    "            assert isinstance(decoder_pointer, nn.Module) and isinstance(\n",
    "                encoder_pointer, nn.Module\n",
    "            ), f\"{decoder_pointer} and {encoder_pointer} have to be of type torch.nn.Module\"\n",
    "            if hasattr(decoder_pointer, \"weight\"):\n",
    "                assert hasattr(encoder_pointer, \"weight\")\n",
    "                encoder_pointer.weight = decoder_pointer.weight\n",
    "                if hasattr(decoder_pointer, \"bias\"):\n",
    "                    assert hasattr(encoder_pointer, \"bias\")\n",
    "                    encoder_pointer.bias = decoder_pointer.bias\n",
    "                return\n",
    "\n",
    "            encoder_modules = encoder_pointer._modules\n",
    "            decoder_modules = decoder_pointer._modules\n",
    "            if len(decoder_modules) > 0:\n",
    "                assert (\n",
    "                    len(encoder_modules) > 0\n",
    "                ), f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n",
    "\n",
    "                all_encoder_weights = set([module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()])\n",
    "                encoder_layer_pos = 0\n",
    "                for name, module in decoder_modules.items():\n",
    "                    if name.isdigit():\n",
    "                        encoder_name = str(int(name) + encoder_layer_pos)\n",
    "                        decoder_name = name\n",
    "                        if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n",
    "                            encoder_modules\n",
    "                        ) != len(decoder_modules):\n",
    "                            # this can happen if the name corresponds to the position in a list module list of layers\n",
    "                            # in this case the decoder has added a cross-attention that the encoder does not have\n",
    "                            # thus skip this step and subtract one layer pos from encoder\n",
    "                            encoder_layer_pos -= 1\n",
    "                            continue\n",
    "                    elif name not in encoder_modules:\n",
    "                        continue\n",
    "                    elif depth > 500:\n",
    "                        raise ValueError(\n",
    "                            \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is a circular dependency between two or more `nn.Modules` of your model.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        decoder_name = encoder_name = name\n",
    "                    tie_encoder_to_decoder_recursively(\n",
    "                        decoder_modules[decoder_name],\n",
    "                        encoder_modules[encoder_name],\n",
    "                        module_name + \"/\" + name,\n",
    "                        uninitialized_encoder_weights,\n",
    "                        depth=depth + 1,\n",
    "                    )\n",
    "                    all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n",
    "\n",
    "                uninitialized_encoder_weights += list(all_encoder_weights)\n",
    "\n",
    "        # tie weights recursively\n",
    "        tie_encoder_to_decoder_recursively(decoder, encoder, base_model_prefix, uninitialized_encoder_weights)\n",
    "        if len(uninitialized_encoder_weights) > 0:\n",
    "            logger.warning(\n",
    "                f\"The following encoder weights were not tied to the decoder {uninitialized_encoder_weights}\"\n",
    "            )\n",
    "\n",
    "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
    "        \"\"\"Tie or clone module weights depending of whether we are using TorchScript or not\"\"\"\n",
    "        if self.config.torchscript:\n",
    "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
    "        else:\n",
    "            output_embeddings.weight = input_embeddings.weight\n",
    "\n",
    "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
    "            output_embeddings.bias.data = torch.nn.functional.pad(\n",
    "                output_embeddings.bias.data,\n",
    "                (\n",
    "                    0,\n",
    "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "                ),\n",
    "                \"constant\",\n",
    "                0,\n",
    "            )\n",
    "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
    "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> torch.nn.Embedding:\n",
    "        \"\"\"\n",
    "        Resizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.\n",
    "\n",
    "        Takes care of tying weights embeddings afterwards if the model class has a :obj:`tie_weights()` method.\n",
    "\n",
    "        Arguments:\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                The number of new tokens in the embedding matrix. Increasing the size will add newly initialized\n",
    "                vectors at the end. Reducing the size will remove vectors from the end. If not provided or :obj:`None`,\n",
    "                just returns a pointer to the input tokens :obj:`torch.nn.Embedding` module of the model without doing\n",
    "                anything.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
    "        \"\"\"\n",
    "        model_embeds = self._resize_token_embeddings(new_num_tokens)\n",
    "        if new_num_tokens is None:\n",
    "            return model_embeds\n",
    "\n",
    "        # Update base model and current model config\n",
    "        self.config.vocab_size = new_num_tokens\n",
    "        self.vocab_size = new_num_tokens\n",
    "\n",
    "        # Tie weights again if needed\n",
    "        self.tie_weights()\n",
    "\n",
    "        return model_embeds\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "\n",
    "        # if word embeddings are not tied, make sure that lm head is resized as well\n",
    "        if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n",
    "            old_lm_head = self.get_output_embeddings()\n",
    "            new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n",
    "            self.set_output_embeddings(new_lm_head)\n",
    "\n",
    "        return self.get_input_embeddings()\n",
    "\n",
    "    def _get_resized_embeddings(\n",
    "        self, old_embeddings: torch.nn.Embedding, new_num_tokens: Optional[int] = None\n",
    "    ) -> torch.nn.Embedding:\n",
    "        \"\"\"\n",
    "        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\n",
    "        initialized vectors at the end. Reducing the size will remove vectors from the end\n",
    "\n",
    "        Args:\n",
    "            old_embeddings (:obj:`torch.nn.Embedding`):\n",
    "                Old embeddings to be resized.\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                New number of tokens in the embedding matrix.\n",
    "\n",
    "                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n",
    "                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens\n",
    "                :obj:`torch.nn.Embedding`` module of the model without doing anything.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if\n",
    "            :obj:`new_num_tokens` is :obj:`None`\n",
    "        \"\"\"\n",
    "        if new_num_tokens is None:\n",
    "            return old_embeddings\n",
    "\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        if old_num_tokens == new_num_tokens:\n",
    "            return old_embeddings\n",
    "\n",
    "        if not isinstance(old_embeddings, nn.Embedding):\n",
    "            raise TypeError(\n",
    "                f\"Old embeddings are of type {type(old_embeddings)}, which is not an instance of {nn.Embedding}.\"\n",
    "                f\"You should either use a different resize function or make sure that `old_embeddings` are an instance of {nn.Embedding}.\"\n",
    "            )\n",
    "\n",
    "        # Build new embeddings\n",
    "        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)\n",
    "\n",
    "        # initialize all new embeddings (in particular added tokens)\n",
    "        self._init_weights(new_embeddings)\n",
    "\n",
    "        # Copy token embeddings from the previous weights\n",
    "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
    "        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n",
    "\n",
    "        return new_embeddings\n",
    "\n",
    "    def _get_resized_lm_head(\n",
    "        self, old_lm_head: torch.nn.Linear, new_num_tokens: Optional[int] = None, transposed: Optional[bool] = False\n",
    "    ) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized\n",
    "        vectors at the end. Reducing the size will remove vectors from the end\n",
    "\n",
    "        Args:\n",
    "            old_lm_head (:obj:`torch.nn.Linear`):\n",
    "                Old lm head liner layer to be resized.\n",
    "            new_num_tokens (:obj:`int`, `optional`):\n",
    "                New number of tokens in the linear matrix.\n",
    "\n",
    "                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n",
    "                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens\n",
    "                :obj:`torch.nn.Linear`` module of the model without doing anything.\n",
    "            transposed (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether ``old_lm_head`` is transposed or not. If True ``old_lm_head.size()`` is ``lm_head_dim,\n",
    "                vocab_size`` else ``vocab_size, lm_head_dim``.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if\n",
    "            :obj:`new_num_tokens` is :obj:`None`\n",
    "        \"\"\"\n",
    "        if new_num_tokens is None:\n",
    "            return old_lm_head\n",
    "\n",
    "        old_num_tokens, old_lm_head_dim = (\n",
    "            old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n",
    "        )\n",
    "\n",
    "        if old_num_tokens == new_num_tokens:\n",
    "            return old_lm_head\n",
    "\n",
    "        if not isinstance(old_lm_head, nn.Linear):\n",
    "            raise TypeError(\n",
    "                f\"Old language model head is of type {type(old_lm_head)}, which is not an instance of {nn.Linear}.\"\n",
    "                f\"You should either use a different resize function or make sure that `old_embeddings` are an instance of {nn.Linear}.\"\n",
    "            )\n",
    "\n",
    "        # Build new lm head\n",
    "        new_lm_head_shape = (old_lm_head_dim, new_num_tokens) if not transposed else (new_num_tokens, old_lm_head_dim)\n",
    "        has_new_lm_head_bias = old_lm_head.bias is not None\n",
    "        new_lm_head = nn.Linear(*new_lm_head_shape, bias=has_new_lm_head_bias).to(self.device)\n",
    "\n",
    "        # initialize new lm head (in particular added tokens)\n",
    "        self._init_weights(new_lm_head)\n",
    "\n",
    "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
    "\n",
    "        # Copy old lm head weights to new lm head\n",
    "        if not transposed:\n",
    "            new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[:num_tokens_to_copy, :]\n",
    "        else:\n",
    "            new_lm_head.weight.data[:, :num_tokens_to_copy] = old_lm_head.weight.data[:, :num_tokens_to_copy]\n",
    "\n",
    "        # Copy bias weights to new lm head\n",
    "        if has_new_lm_head_bias:\n",
    "            new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[:num_tokens_to_copy]\n",
    "\n",
    "        return new_lm_head\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes and prunes weights if needed.\n",
    "        \"\"\"\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Prune heads if needed\n",
    "        if self.config.pruned_heads:\n",
    "            self.prune_heads(self.config.pruned_heads)\n",
    "\n",
    "        # Tie weights if needed\n",
    "        self.tie_weights()\n",
    "\n",
    "    def prune_heads(self, heads_to_prune: Dict[int, List[int]]):\n",
    "        \"\"\"\n",
    "        Prunes heads of the base model.\n",
    "\n",
    "        Arguments:\n",
    "            heads_to_prune (:obj:`Dict[int, List[int]]`):\n",
    "                Dictionary with keys being selected layer indices (:obj:`int`) and associated values being the list of\n",
    "                heads to prune in said layer (list of :obj:`int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads\n",
    "                0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
    "        \"\"\"\n",
    "        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n",
    "            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n",
    "\n",
    "        self.base_model._prune_heads(heads_to_prune)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
    "        \"\"\"\n",
    "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
    "        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "\n",
    "        Arguments:\n",
    "            save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
    "                Directory to which to save. Will be created if it doesn't exist.\n",
    "        \"\"\"\n",
    "        if os.path.isfile(save_directory):\n",
    "            logger.error(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n",
    "            return\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Only save the model itself if we are using distributed training\n",
    "        model_to_save = self.module if hasattr(self, \"module\") else self\n",
    "\n",
    "        # Attach architecture to the config\n",
    "        model_to_save.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "        state_dict = model_to_save.state_dict()\n",
    "\n",
    "        # Handle the case where some state_dict keys shouldn't be saved\n",
    "        if self._keys_to_ignore_on_save is not None:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if k not in self._keys_to_ignore_on_save}\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "\n",
    "        if getattr(self.config, \"xla_device\", False) and is_torch_tpu_available():\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            if xm.is_master_ordinal():\n",
    "                # Save configuration file\n",
    "                model_to_save.config.save_pretrained(save_directory)\n",
    "            # xm.save takes care of saving only from master\n",
    "            xm.save(state_dict, output_model_file)\n",
    "        else:\n",
    "            model_to_save.config.save_pretrained(save_directory)\n",
    "            torch.save(state_dict, output_model_file)\n",
    "\n",
    "        logger.info(\"Model weights saved in {}\".format(output_model_file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        r\"\"\"\n",
    "        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
    "\n",
    "        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated). To\n",
    "        train the model, you should first set it back in training mode with ``model.train()``.\n",
    "\n",
    "        The warning `Weights from XXX not initialized from pretrained model` means that the weights of XXX do not come\n",
    "        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
    "        task.\n",
    "\n",
    "        The warning `Weights from XXX not used in YYY` means that the layer XXX is not used by YYY, therefore those\n",
    "        weights are discarded.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
    "                Can be either:\n",
    "\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
    "                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
    "                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
    "                    - :obj:`None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
    "                      arguments ``config`` and ``state_dict``).\n",
    "            model_args (sequence of positional arguments, `optional`):\n",
    "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n",
    "            config (:obj:`Union[PretrainedConfig, str, os.PathLike]`, `optional`):\n",
    "                Can be either:\n",
    "\n",
    "                    - an instance of a class derived from :class:`~transformers.PretrainedConfig`,\n",
    "                    - a string or path valid as input to :func:`~transformers.PretrainedConfig.from_pretrained`.\n",
    "\n",
    "                Configuration for the model to use instead of an automatically loaded configuation. Configuration can\n",
    "                be automatically loaded when:\n",
    "\n",
    "                    - The model is a model provided by the library (loaded with the `model id` string of a pretrained\n",
    "                      model).\n",
    "                    - The model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded\n",
    "                      by supplying the save directory.\n",
    "                    - The model is loaded by supplying a local directory as ``pretrained_model_name_or_path`` and a\n",
    "                      configuration JSON file named `config.json` is found in the directory.\n",
    "            state_dict (:obj:`Dict[str, torch.Tensor]`, `optional`):\n",
    "                A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
    "\n",
    "                This option can be used if you want to create a model from a pretrained configuration but load your own\n",
    "                weights. In this case though, you should check if using\n",
    "                :func:`~transformers.PreTrainedModel.save_pretrained` and\n",
    "                :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
    "            cache_dir (:obj:`Union[str, os.PathLike]`, `optional`):\n",
    "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
    "                standard cache should not be used.\n",
    "            from_tf (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
    "                ``pretrained_model_name_or_path`` argument).\n",
    "            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
    "                cached versions if they exist.\n",
    "            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
    "                file exists.\n",
    "            proxies (:obj:`Dict[str, str], `optional`):\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
    "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
    "            output_loading_info(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
    "            local_files_only(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to only look at local files (i.e., do not try to download the model).\n",
    "            use_auth_token (:obj:`str` or `bool`, `optional`):\n",
    "                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
    "                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
    "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
    "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
    "                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
    "                identifier allowed by git.\n",
    "            mirror(:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
    "                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
    "                Please refer to the mirror site for more information.\n",
    "            kwargs (remaining dictionary of keyword arguments, `optional`):\n",
    "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
    "                :obj:`output_attentions=True`). Behaves differently depending on whether a ``config`` is provided or\n",
    "                automatically loaded:\n",
    "\n",
    "                    - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the\n",
    "                      underlying model's ``__init__`` method (we assume all relevant updates to the configuration have\n",
    "                      already been done)\n",
    "                    - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class\n",
    "                      initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of\n",
    "                      ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute\n",
    "                      with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration\n",
    "                      attribute will be passed to the underlying model's ``__init__`` function.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import BertConfig, BertModel\n",
    "            >>> # Download model and configuration from huggingface.co and cache.\n",
    "            >>> model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            >>> # Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).\n",
    "            >>> model = BertModel.from_pretrained('./test/saved_model/')\n",
    "            >>> # Update configuration during loading.\n",
    "            >>> model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "            >>> assert model.config.output_attentions == True\n",
    "            >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
    "            >>> config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
    "            >>> model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
    "        \"\"\"\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        state_dict = kwargs.pop(\"state_dict\", None)\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        from_tf = kwargs.pop(\"from_tf\", False)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
    "        revision = kwargs.pop(\"revision\", None)\n",
    "        mirror = kwargs.pop(\"mirror\", None)\n",
    "\n",
    "        # Load config if we don't provide a configuration\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            config_path = config if config is not None else pretrained_model_name_or_path\n",
    "            config, model_kwargs = cls.config_class.from_pretrained(\n",
    "                config_path,\n",
    "                *model_args,\n",
    "                cache_dir=cache_dir,\n",
    "                return_unused_kwargs=True,\n",
    "                force_download=force_download,\n",
    "                resume_download=resume_download,\n",
    "                proxies=proxies,\n",
    "                local_files_only=local_files_only,\n",
    "                use_auth_token=use_auth_token,\n",
    "                revision=revision,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            model_kwargs = kwargs\n",
    "\n",
    "        # Load model\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
    "                    # Load from a TF 1.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
    "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
    "                    # Load from a TF 2.0 checkpoint in priority if from_tf\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
    "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
    "                    # Load from a PyTorch checkpoint\n",
    "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
    "                else:\n",
    "                    raise EnvironmentError(\n",
    "                        \"Error no file named {} found in directory {} or `from_tf` set to False\".format(\n",
    "                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + \".index\"],\n",
    "                            pretrained_model_name_or_path,\n",
    "                        )\n",
    "                    )\n",
    "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                archive_file = pretrained_model_name_or_path\n",
    "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
    "                assert (\n",
    "                    from_tf\n",
    "                ), \"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\".format(\n",
    "                    pretrained_model_name_or_path + \".index\"\n",
    "                )\n",
    "                archive_file = pretrained_model_name_or_path + \".index\"\n",
    "            else:\n",
    "                archive_file = hf_bucket_url(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    filename=(TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME),\n",
    "                    revision=revision,\n",
    "                    mirror=mirror,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                # Load from URL or cache if already cached\n",
    "                resolved_archive_file = cached_path(\n",
    "                    archive_file,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_download=force_download,\n",
    "                    proxies=proxies,\n",
    "                    resume_download=resume_download,\n",
    "                    local_files_only=local_files_only,\n",
    "                    use_auth_token=use_auth_token,\n",
    "                )\n",
    "            except EnvironmentError as err:\n",
    "                logger.error(err)\n",
    "                msg = (\n",
    "                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n",
    "                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n",
    "                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n",
    "                )\n",
    "                raise EnvironmentError(msg)\n",
    "\n",
    "            if resolved_archive_file == archive_file:\n",
    "                logger.info(\"loading weights file {}\".format(archive_file))\n",
    "            else:\n",
    "                logger.info(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n",
    "        else:\n",
    "            resolved_archive_file = None\n",
    "\n",
    "        config.name_or_path = pretrained_model_name_or_path\n",
    "\n",
    "        # Instantiate model.\n",
    "        model = cls(config, *model_args, **model_kwargs)\n",
    "\n",
    "        if state_dict is None and not from_tf:\n",
    "            try:\n",
    "                state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
    "            except Exception:\n",
    "                raise OSError(\n",
    "                    f\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\n",
    "                    f\"at '{resolved_archive_file}'\"\n",
    "                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
    "                )\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "\n",
    "        if from_tf:\n",
    "            if resolved_archive_file.endswith(\".index\"):\n",
    "                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n",
    "                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n",
    "            else:\n",
    "                # Load from our TensorFlow 2.0 checkpoints\n",
    "                try:\n",
    "                    from .modeling_tf_pytorch_utils import load_tf2_checkpoint_in_pytorch_model\n",
    "\n",
    "                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n",
    "                except ImportError:\n",
    "                    logger.error(\n",
    "                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"\n",
    "                        \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "                    )\n",
    "                    raise\n",
    "        else:\n",
    "            # Convert old format to new format if needed from a PyTorch state_dict\n",
    "            old_keys = []\n",
    "            new_keys = []\n",
    "            for key in state_dict.keys():\n",
    "                new_key = None\n",
    "                if \"gamma\" in key:\n",
    "                    new_key = key.replace(\"gamma\", \"weight\")\n",
    "                if \"beta\" in key:\n",
    "                    new_key = key.replace(\"beta\", \"bias\")\n",
    "                if new_key:\n",
    "                    old_keys.append(key)\n",
    "                    new_keys.append(new_key)\n",
    "            for old_key, new_key in zip(old_keys, new_keys):\n",
    "                state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "            # copy state_dict so _load_from_state_dict can modify it\n",
    "            metadata = getattr(state_dict, \"_metadata\", None)\n",
    "            state_dict = state_dict.copy()\n",
    "            if metadata is not None:\n",
    "                state_dict._metadata = metadata\n",
    "\n",
    "            # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n",
    "            # so we need to apply the function recursively.\n",
    "            def load(module: nn.Module, prefix=\"\"):\n",
    "                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "                module._load_from_state_dict(\n",
    "                    state_dict,\n",
    "                    prefix,\n",
    "                    local_metadata,\n",
    "                    True,\n",
    "                    missing_keys,\n",
    "                    unexpected_keys,\n",
    "                    error_msgs,\n",
    "                )\n",
    "                for name, child in module._modules.items():\n",
    "                    if child is not None:\n",
    "                        load(child, prefix + name + \".\")\n",
    "\n",
    "            # Make sure we are able to load base models as well as derived models (with heads)\n",
    "            start_prefix = \"\"\n",
    "            model_to_load = model\n",
    "            has_prefix_module = any(s.startswith(cls.base_model_prefix) for s in state_dict.keys())\n",
    "            if not hasattr(model, cls.base_model_prefix) and has_prefix_module:\n",
    "                start_prefix = cls.base_model_prefix + \".\"\n",
    "            if hasattr(model, cls.base_model_prefix) and not has_prefix_module:\n",
    "                model_to_load = getattr(model, cls.base_model_prefix)\n",
    "\n",
    "            load(model_to_load, prefix=start_prefix)\n",
    "\n",
    "            if model.__class__.__name__ != model_to_load.__class__.__name__:\n",
    "                base_model_state_dict = model_to_load.state_dict().keys()\n",
    "                head_model_state_dict_without_base_prefix = [\n",
    "                    key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n",
    "                ]\n",
    "                missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n",
    "\n",
    "            # Some models may have keys that are not in the state by design, removing them before needlessly warning\n",
    "            # the user.\n",
    "            if cls._keys_to_ignore_on_load_missing is not None:\n",
    "                for pat in cls._keys_to_ignore_on_load_missing:\n",
    "                    missing_keys = [k for k in missing_keys if re.search(pat, k) is None]\n",
    "\n",
    "            if cls._keys_to_ignore_on_load_unexpected is not None:\n",
    "                for pat in cls._keys_to_ignore_on_load_unexpected:\n",
    "                    unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n",
    "\n",
    "            if len(unexpected_keys) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when \"\n",
    "                    f\"initializing {model.__class__.__name__}: {unexpected_keys}\\n\"\n",
    "                    f\"- This IS expected if you are initializing {model.__class__.__name__} from the checkpoint of a model trained on another task \"\n",
    "                    f\"or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n\"\n",
    "                    f\"- This IS NOT expected if you are initializing {model.__class__.__name__} from the checkpoint of a model that you expect \"\n",
    "                    f\"to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n",
    "            if len(missing_keys) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at {pretrained_model_name_or_path} \"\n",
    "                    f\"and are newly initialized: {missing_keys}\\n\"\n",
    "                    f\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at {pretrained_model_name_or_path}.\\n\"\n",
    "                    f\"If your task is similar to the task the model of the checkpoint was trained on, \"\n",
    "                    f\"you can already use {model.__class__.__name__} for predictions without further training.\"\n",
    "                )\n",
    "            if len(error_msgs) > 0:\n",
    "                raise RuntimeError(\n",
    "                    \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n",
    "                        model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n",
    "                    )\n",
    "                )\n",
    "        # make sure token embedding weights are still tied if needed\n",
    "        model.tie_weights()\n",
    "\n",
    "        # Set model in evaluation mode to deactivate DropOut modules by default\n",
    "        model.eval()\n",
    "\n",
    "        if output_loading_info:\n",
    "            loading_info = {\n",
    "                \"missing_keys\": missing_keys,\n",
    "                \"unexpected_keys\": unexpected_keys,\n",
    "                \"error_msgs\": error_msgs,\n",
    "            }\n",
    "            return model, loading_info\n",
    "\n",
    "        if hasattr(config, \"xla_device\") and config.xla_device and is_torch_tpu_available():\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            model = xm.send_cpu_data_to_device(model, xm.xla_device())\n",
    "            model.to(xm.xla_device())\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "\n",
    "    Args:\n",
    "        nf (:obj:`int`): The number of output features.\n",
    "        nx (:obj:`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, nx):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerStartLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD start logits from sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The start logits for SQuAD.\n",
    "        \"\"\"\n",
    "        x = self.dense(hidden_states).squeeze(-1)\n",
    "\n",
    "        if p_mask is not None:\n",
    "            if next(self.parameters()).dtype == torch.float16:\n",
    "                x = x * (1 - p_mask) - 65500 * p_mask\n",
    "            else:\n",
    "                x = x * (1 - p_mask) - 1e30 * p_mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerEndLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD end logits from sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the\n",
    "            :obj:`layer_norm_eps` to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_states: Optional[torch.FloatTensor] = None,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        p_mask: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):\n",
    "                The hidden states of the first tokens for the labeled span.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                The position of the first token for the labeled span.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,\n",
    "            ``start_positions`` overrides ``start_states``.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The end logits for SQuAD.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            start_states is not None or start_positions is not None\n",
    "        ), \"One of start_states, start_positions should be not None\"\n",
    "        if start_positions is not None:\n",
    "            slen, hsz = hidden_states.shape[-2:]\n",
    "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n",
    "            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n",
    "\n",
    "        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n",
    "        x = self.activation(x)\n",
    "        x = self.LayerNorm(x)\n",
    "        x = self.dense_1(x).squeeze(-1)\n",
    "\n",
    "        if p_mask is not None:\n",
    "            if next(self.parameters()).dtype == torch.float16:\n",
    "                x = x * (1 - p_mask) - 65500 * p_mask\n",
    "            else:\n",
    "                x = x * (1 - p_mask) - 1e30 * p_mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolerAnswerClass(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_states: Optional[torch.FloatTensor] = None,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        cls_index: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):\n",
    "                The hidden states of the first tokens for the labeled span.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                The position of the first token for the labeled span.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,\n",
    "            ``start_positions`` overrides ``start_states``.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The SQuAD 2.0 answer class.\n",
    "        \"\"\"\n",
    "        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n",
    "        hsz = hidden_states.shape[-1]\n",
    "        assert (\n",
    "            start_states is not None or start_positions is not None\n",
    "        ), \"One of start_states, start_positions should be not None\"\n",
    "        if start_positions is not None:\n",
    "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n",
    "\n",
    "        if cls_index is not None:\n",
    "            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n",
    "            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n",
    "        else:\n",
    "            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n",
    "\n",
    "        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_1(x).squeeze(-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SquadHeadOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of question answering models using a :class:`~transformers.modeling_utils.SQuADHead`.\n",
    "\n",
    "    Args:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned if both :obj:`start_positions` and :obj:`end_positions` are provided):\n",
    "            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n",
    "            losses.\n",
    "        start_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n",
    "        start_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Indices for the top config.start_n_top start token possibilities (beam-search).\n",
    "        end_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities\n",
    "            (beam-search).\n",
    "        end_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
    "        cls_logits (``torch.FloatTensor`` of shape ``(batch_size,)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n",
    "            Log probabilities for the ``is_impossible`` label of the answers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    start_top_log_probs: Optional[torch.FloatTensor] = None\n",
    "    start_top_index: Optional[torch.LongTensor] = None\n",
    "    end_top_log_probs: Optional[torch.FloatTensor] = None\n",
    "    end_top_index: Optional[torch.LongTensor] = None\n",
    "    cls_logits: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "class SQuADHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    A SQuAD head inspired by XLNet.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the\n",
    "            :obj:`layer_norm_eps` to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.start_n_top = config.start_n_top\n",
    "        self.end_n_top = config.end_n_top\n",
    "\n",
    "        self.start_logits = PoolerStartLogits(config)\n",
    "        self.end_logits = PoolerEndLogits(config)\n",
    "        self.answer_class = PoolerAnswerClass(config)\n",
    "\n",
    "    @replace_return_docstrings(output_type=SquadHeadOutput, config_class=PretrainedConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        start_positions: Optional[torch.LongTensor] = None,\n",
    "        end_positions: Optional[torch.LongTensor] = None,\n",
    "        cls_index: Optional[torch.LongTensor] = None,\n",
    "        is_impossible: Optional[torch.LongTensor] = None,\n",
    "        p_mask: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: bool = False,\n",
    "    ) -> Union[SquadHeadOutput, Tuple[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):\n",
    "                Final hidden states of the model on the sequence tokens.\n",
    "            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Positions of the first token for the labeled span.\n",
    "            end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Positions of the last token for the labeled span.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.\n",
    "            is_impossible (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "                Whether the question has a possible answer in the paragraph or not.\n",
    "            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):\n",
    "                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n",
    "                should be masked.\n",
    "            return_dict (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n",
    "            for x in (start_positions, end_positions, cls_index, is_impossible):\n",
    "                if x is not None and x.dim() > 1:\n",
    "                    x.squeeze_(-1)\n",
    "\n",
    "            # during training, compute the end logits based on the ground truth of the start position\n",
    "            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "            if cls_index is not None and is_impossible is not None:\n",
    "                # Predict answerability from the representation of CLS and START\n",
    "                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n",
    "                loss_fct_cls = nn.BCEWithLogitsLoss()\n",
    "                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n",
    "\n",
    "                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n",
    "                total_loss += cls_loss * 0.5\n",
    "\n",
    "            return SquadHeadOutput(loss=total_loss) if return_dict else (total_loss,)\n",
    "\n",
    "        else:\n",
    "            # during inference, compute the end logits based on beam search\n",
    "            bsz, slen, hsz = hidden_states.size()\n",
    "            start_log_probs = F.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n",
    "\n",
    "            start_top_log_probs, start_top_index = torch.topk(\n",
    "                start_log_probs, self.start_n_top, dim=-1\n",
    "            )  # shape (bsz, start_n_top)\n",
    "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n",
    "            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n",
    "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n",
    "\n",
    "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n",
    "                start_states\n",
    "            )  # shape (bsz, slen, start_n_top, hsz)\n",
    "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
    "            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n",
    "            end_log_probs = F.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n",
    "\n",
    "            end_top_log_probs, end_top_index = torch.topk(\n",
    "                end_log_probs, self.end_n_top, dim=1\n",
    "            )  # shape (bsz, end_n_top, start_n_top)\n",
    "            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n",
    "            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n",
    "\n",
    "            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n",
    "            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n",
    "\n",
    "            if not return_dict:\n",
    "                return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n",
    "            else:\n",
    "                return SquadHeadOutput(\n",
    "                    start_top_log_probs=start_top_log_probs,\n",
    "                    start_top_index=start_top_index,\n",
    "                    end_top_log_probs=end_top_log_probs,\n",
    "                    end_top_index=end_top_index,\n",
    "                    cls_logits=cls_logits,\n",
    "                )\n",
    "\n",
    "\n",
    "class SequenceSummary(nn.Module):\n",
    "    r\"\"\"\n",
    "    Compute a single vector summary of a sequence hidden states.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.PretrainedConfig`):\n",
    "            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n",
    "            config class of your model for the default values it uses):\n",
    "\n",
    "            - **summary_type** (:obj:`str`) -- The method to use to make this summary. Accepted values are:\n",
    "\n",
    "                - :obj:`\"last\"` -- Take the last token hidden state (like XLNet)\n",
    "                - :obj:`\"first\"` -- Take the first token hidden state (like Bert)\n",
    "                - :obj:`\"mean\"` -- Take the mean of all tokens hidden states\n",
    "                - :obj:`\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n",
    "                - :obj:`\"attn\"` -- Not implemented now, use multi-head attention\n",
    "\n",
    "            - **summary_use_proj** (:obj:`bool`) -- Add a projection after the vector extraction.\n",
    "            - **summary_proj_to_labels** (:obj:`bool`) -- If :obj:`True`, the projection outputs to\n",
    "              :obj:`config.num_labels` classes (otherwise to :obj:`config.hidden_size`).\n",
    "            - **summary_activation** (:obj:`Optional[str]`) -- Set to :obj:`\"tanh\"` to add a tanh activation to the\n",
    "              output, another string or :obj:`None` will add no activation.\n",
    "            - **summary_first_dropout** (:obj:`float`) -- Optional dropout probability before the projection and\n",
    "              activation.\n",
    "            - **summary_last_dropout** (:obj:`float`)-- Optional dropout probability after the projection and\n",
    "              activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.summary_type = getattr(config, \"summary_type\", \"last\")\n",
    "        if self.summary_type == \"attn\":\n",
    "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
    "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
    "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        activation_string = getattr(config, \"summary_activation\", None)\n",
    "        self.activation: Callable = get_activation(activation_string) if activation_string else Identity()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Compute a single vector summary of a sequence hidden states.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`[batch_size, seq_len, hidden_size]`):\n",
    "                The hidden states of the last layer.\n",
    "            cls_index (:obj:`torch.LongTensor` of shape :obj:`[batch_size]` or :obj:`[batch_size, ...]` where ... are optional leading dimensions of :obj:`hidden_states`, `optional`):\n",
    "                Used if :obj:`summary_type == \"cls_index\"` and takes the last token of the sequence as classification\n",
    "                token.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.FloatTensor`: The summary of the sequence hidden states.\n",
    "        \"\"\"\n",
    "        if self.summary_type == \"last\":\n",
    "            output = hidden_states[:, -1]\n",
    "        elif self.summary_type == \"first\":\n",
    "            output = hidden_states[:, 0]\n",
    "        elif self.summary_type == \"mean\":\n",
    "            output = hidden_states.mean(dim=1)\n",
    "        elif self.summary_type == \"cls_index\":\n",
    "            if cls_index is None:\n",
    "                cls_index = torch.full_like(\n",
    "                    hidden_states[..., :1, :],\n",
    "                    hidden_states.shape[-2] - 1,\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "            else:\n",
    "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
    "                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n",
    "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
    "            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n",
    "        elif self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def prune_linear_layer(layer: torch.nn.Linear, index: torch.LongTensor, dim: int = 0) -> torch.nn.Linear:\n",
    "    \"\"\"\n",
    "    Prune a linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:obj:`torch.nn.Linear`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.nn.Linear`: The pruned layer as a new layer with :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if layer.bias is not None:\n",
    "        if dim == 1:\n",
    "            b = layer.bias.clone().detach()\n",
    "        else:\n",
    "            b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    if layer.bias is not None:\n",
    "        new_layer.bias.requires_grad = False\n",
    "        new_layer.bias.copy_(b.contiguous())\n",
    "        new_layer.bias.requires_grad = True\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "def prune_conv1d_layer(layer: Conv1D, index: torch.LongTensor, dim: int = 1) -> Conv1D:\n",
    "    \"\"\"\n",
    "    Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights\n",
    "    are transposed.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:class:`~transformers.modeling_utils.Conv1D`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`, defaults to 1): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if dim == 0:\n",
    "        b = layer.bias.clone().detach()\n",
    "    else:\n",
    "        b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    new_layer.bias.requires_grad = False\n",
    "    new_layer.bias.copy_(b.contiguous())\n",
    "    new_layer.bias.requires_grad = True\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "def prune_layer(\n",
    "    layer: Union[torch.nn.Linear, Conv1D], index: torch.LongTensor, dim: Optional[int] = None\n",
    ") -> Union[torch.nn.Linear, Conv1D]:\n",
    "    \"\"\"\n",
    "    Prune a Conv1D or linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (:obj:`Union[torch.nn.Linear, Conv1D]`): The layer to prune.\n",
    "        index (:obj:`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (:obj:`int`, `optional`): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.nn.Linear` or :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with\n",
    "        :obj:`requires_grad=True`.\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n",
    "    elif isinstance(layer, Conv1D):\n",
    "        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n",
    "    else:\n",
    "        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))\n",
    "\n",
    "\n",
    "def apply_chunking_to_forward(\n",
    "    forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function chunks the :obj:`input_tensors` into smaller input tensor parts of size :obj:`chunk_size` over the\n",
    "    dimension :obj:`chunk_dim`. It then applies a layer :obj:`forward_fn` to each chunk independently to save memory.\n",
    "\n",
    "    If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this function will yield the same result as\n",
    "    directly applying :obj:`forward_fn` to :obj:`input_tensors`.\n",
    "\n",
    "    Args:\n",
    "        forward_fn (:obj:`Callable[..., torch.Tensor]`):\n",
    "            The forward function of the model.\n",
    "        chunk_size (:obj:`int`):\n",
    "            The chunk size of a chunked tensor: :obj:`num_chunks = len(input_tensors[0]) / chunk_size`.\n",
    "        chunk_dim (:obj:`int`):\n",
    "            The dimension over which the :obj:`input_tensors` should be chunked.\n",
    "        input_tensors (:obj:`Tuple[torch.Tensor]`):\n",
    "            The input tensors of ``forward_fn`` which will be chunked\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor`: A tensor with the same shape as the :obj:`forward_fn` would have given if applied`.\n",
    "\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        # rename the usual forward() fn to forward_chunk()\n",
    "        def forward_chunk(self, hidden_states):\n",
    "            hidden_states = self.decoder(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "        # implement a chunked forward function\n",
    "        def forward(self, hidden_states):\n",
    "            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(input_tensors) > 0, \"{} has to be a tuple/list of tensors\".format(input_tensors)\n",
    "    tensor_shape = input_tensors[0].shape[chunk_dim]\n",
    "    assert all(\n",
    "        input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
    "    ), \"All input tenors have to be of the same shape\"\n",
    "\n",
    "    # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
    "    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
    "    assert num_args_in_forward_chunk_fn == len(\n",
    "        input_tensors\n",
    "    ), \"forward_chunk_fn expects {} arguments, but only {} input tensors are given\".format(\n",
    "        num_args_in_forward_chunk_fn, len(input_tensors)\n",
    "    )\n",
    "\n",
    "    if chunk_size > 0:\n",
    "        assert (\n",
    "            input_tensors[0].shape[chunk_dim] % chunk_size == 0\n",
    "        ), \"The dimension to be chunked {} has to be a multiple of the chunk size {}\".format(\n",
    "            input_tensors[0].shape[chunk_dim], chunk_size\n",
    "        )\n",
    "\n",
    "        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
    "\n",
    "        # chunk input tensor into tuples\n",
    "        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
    "        # apply forward fn to every tuple\n",
    "        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
    "        # concatenate output at same dimension\n",
    "        return torch.cat(output_chunks, dim=chunk_dim)\n",
    "\n",
    "    return forward_fn(*input_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:03.063240Z",
     "start_time": "2021-02-10T06:54:02.932990Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch BERT model. \"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "# from ...file_utils import (\n",
    "#     ModelOutput,\n",
    "#     add_code_sample_docstrings,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "\n",
    "# from ...modeling_outputs import (\n",
    "#     BaseModelOutputWithPastAndCrossAttentions,\n",
    "#     BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "#     CausalLMOutputWithCrossAttentions,\n",
    "#     MaskedLMOutput,\n",
    "#     MultipleChoiceModelOutput,\n",
    "#     NextSentencePredictorOutput,\n",
    "#     QuestionAnsweringModelOutput,\n",
    "#     SequenceClassifierOutput,\n",
    "#     TokenClassifierOutput,\n",
    "# )\n",
    "# from ...modeling_utils import (\n",
    "#     PreTrainedModel,\n",
    "#     apply_chunking_to_forward,\n",
    "#     find_pruneable_heads_and_indices,\n",
    "#     prune_linear_layer,\n",
    "# )\n",
    "\n",
    "\n",
    "# from ...utils import logging\n",
    "# from .configuration_bert import BertConfig\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"BertConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
    "\n",
    "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-large-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-chinese\",\n",
    "    \"bert-base-german-cased\",\n",
    "    \"bert-large-uncased-whole-word-masking\",\n",
    "    \"bert-large-cased-whole-word-masking\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-base-cased-finetuned-mrpc\",\n",
    "    \"bert-base-german-dbmdz-cased\",\n",
    "    \"bert-base-german-dbmdz-uncased\",\n",
    "    \"cl-tohoku/bert-base-japanese\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
    "    \"wietsedv/bert-base-dutch-cased\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "]\n",
    "\n",
    "\n",
    "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
    "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
    "    try:\n",
    "        import re\n",
    "\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(\n",
    "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
    "            for n in name\n",
    "        ):\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "                pointer = getattr(pointer, \"bias\")\n",
    "            elif scope_names[0] == \"output_weights\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"squad\":\n",
    "                pointer = getattr(pointer, \"classifier\")\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == \"_embeddings\":\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        elif m_name == \"kernel\":\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            assert (\n",
    "                pointer.shape == array.shape\n",
    "            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"crossattention\"\n",
    "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warn(\n",
    "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                        \"`use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BertForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of :class:`~transformers.BertForPreTraining`.\n",
    "\n",
    "    Args:\n",
    "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "BERT_START_DOCSTRING = r\"\"\"\n",
    "\n",
    "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
    "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
    "    pruning heads etc.)\n",
    "\n",
    "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
    "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
    "    general usage and behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
    "            weights.\n",
    "\"\"\"\n",
    "\n",
    "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
    "            1]``:\n",
    "\n",
    "            - 0 corresponds to a `sentence A` token,\n",
    "            - 1 corresponds to a `sentence B` token.\n",
    "\n",
    "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
    "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
    "            config.max_position_embeddings - 1]``.\n",
    "\n",
    "            `What are position IDs? <../glossary.html#position-ids>`_\n",
    "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
    "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
    "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
    "            vectors than the model's internal embedding lookup matrix.\n",
    "        output_attentions (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (:obj:`bool`, `optional`):\n",
    "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n",
    "    sentence prediction (classification)` head.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        next_sentence_label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertForPreTraining\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "\n",
    "            >>> prediction_logits = outputs.prediction_logits\n",
    "            >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n",
    ")\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if not config.is_decoder:\n",
    "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "            >>> config.is_decoder = True\n",
    "            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
    "\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "\n",
    "            >>> prediction_logits = outputs.logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\n",
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=MaskedLMOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "            >>> import torch\n",
    "\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "            >>> logits = outputs.logits\n",
    "            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
    "        \"\"\"\n",
    "\n",
    "        if \"next_sentence_label\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"next_sentence_label\")\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        seq_relationship_scores = self.cls(pooled_output)\n",
    "\n",
    "        next_sentence_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (seq_relationship_scores,) + outputs[2:]\n",
    "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
    "\n",
    "        return NextSentencePredictorOutput(\n",
    "            loss=next_sentence_loss,\n",
    "            logits=seq_relationship_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
    "    output) e.g. for GLUE tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
    "    softmax) e.g. for RocStories/SWAG tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForMultipleChoice(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
    "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
    "            :obj:`input_ids` above)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
    "        inputs_embeds = (\n",
    "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
    "            if inputs_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, num_choices)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reshaped_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MultipleChoiceModelOutput(\n",
    "            loss=loss,\n",
    "            logits=reshaped_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
    "    Named-Entity-Recognition (NER) tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=TokenClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
    "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T06:54:04.408949Z",
     "start_time": "2021-02-10T06:54:03.064916Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"/Users/yongdeng/Downloads/ä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹/chinese-roberta-wwm-ext/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
